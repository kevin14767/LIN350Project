{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIN350 Course Project - The Language of Immigration Politics: Terminology Differences Across Party Lines in Congressional Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way I usually run jupyter notebooks is opening the anaconda prompt terminal and running the command *jupyter notebook* from there I go to visual studio and click on select kernel -> existing jupyter server -> localhost or you can copy and paste the url of the tab that opened up with the *jupyter notebook* command and then click on python and that should be it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep track of the work we're doing together we can use a github repository to update changes and sync up our work. The usual workflow for this should be.\n",
    "1. Any changes you have in your laptop can be added to the repository with \"git add ./\" from the terminal the notebook is in\n",
    "2. After adding the files and changes you can use \"git commit -m 'message here'\" For the message make sure its in quotations and it can be anything\n",
    "3. After adding and commiting you can \"git push\" which pushes ur changes to the repository\n",
    "4. Let's say there's changes in the repository that are not in your laptop you can fetch them with \"git pull\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other setup you might need to do is set environement variables in local computer since we don't want to share that in the repository for privacy issues. So to do this you would run commands in your notebook to set it up. I'll show you\n",
    "1. running \"%env\" in a code block will show you all the environment variables in the jupyter environment\n",
    "2. to set up the enviroment variable for our project run the command \"%env API_KEY=apikeyfromourgoogledocs\"\n",
    "3. After that running the first cell of code will setup the api key to be used as API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: API_KEY=qAyZUrTJs4fdKIPefgekdiMQrCchdt979fIo58M1\n"
     ]
    }
   ],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Record Data Collector - Very simple for now, simple text data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import xlsxwriter\n",
    "\n",
    "# create directories for data storage\n",
    "os.makedirs('data/expanded_data_now', exist_ok=True)\n",
    "\n",
    "# set your API key here (get one from https://api.data.gov/signup/)\n",
    "API_KEY = os.environ[\"API_KEY\"]\n",
    "\n",
    "# define date ranges for your study (immigration debates 2018-2023)\n",
    "date_ranges = [\n",
    "    # 2019 - Border wall government shutdown -> for now \n",
    "    (\"2019-01-01\", \"2019-01-31\"),\n",
    "    \n",
    "    # Government shutdown over border wall funding\n",
    "    (\"2018-12-15\", \"2018-12-31\"),\n",
    "\n",
    "    # DACA debates\n",
    "    (\"2017-09-01\", \"2017-10-15\"),\n",
    "    (\"2018-01-15\", \"2018-02-15\"),\n",
    "    \n",
    "    # Border surge discussions\n",
    "    (\"2019-03-01\", \"2019-04-15\"),\n",
    "    \n",
    "    # Election year immigration discussions\n",
    "    (\"2020-01-15\", \"2020-02-15\"),\n",
    "    (\"2020-09-01\", \"2020-10-15\"),\n",
    "    \n",
    "    # Biden administration policy changes\n",
    "    (\"2021-01-20\", \"2021-03-01\")\n",
    "]\n",
    "# immigration-related terms with more precise matching\n",
    "immigration_terms = {\n",
    "    # regular terms - can appear within other words\n",
    "    'immigration': r'immigration',\n",
    "    'immigrant': r'immigrant',\n",
    "    'migrant': r'migrant',\n",
    "    'citizenship': r'citizenship',\n",
    "    'deportation': r'deportation',\n",
    "    \n",
    "    # terms that need word boundary checks\n",
    "    'border': r'\\b(?:border|borders)\\b',\n",
    "    'asylum': r'\\basylum\\b',\n",
    "    'refugee': r'\\b(?:refugee|refugees)\\b',\n",
    "    'undocumented': r'\\bundocumented\\b',\n",
    "    'illegal alien': r'\\billegal\\s+alien',\n",
    "    'unauthorized': r'\\bunauthorized\\b',\n",
    "    'wall': r'\\bwall\\b',\n",
    "    'daca': r'\\bdaca\\b',\n",
    "    'dreamer': r'\\b(?:dreamer|dreamers)\\b',\n",
    "    'visa': r'\\bvisa\\b',\n",
    "    'detention': r'\\bdetention\\b',\n",
    "    \n",
    "    # phrases\n",
    "    'family separation': r'family\\s+separation',\n",
    "    'child detention': r'child\\s+detention',\n",
    "    'border security': r'border\\s+security',\n",
    "    'border crisis': r'border\\s+crisis',\n",
    "    'path to citizenship': r'path\\s+to\\s+citizenship',\n",
    "    'amnesty': r'\\bamnesty\\b',\n",
    "    'caravan': r'\\bcaravan\\b',\n",
    "    \n",
    "    # specific entities\n",
    "    'mexico': r'\\bmexico\\b',\n",
    "    'ice': r'\\b(?:ice|immigration and customs enforcement)\\b',  # Only match whole word \"ice\"\n",
    "    'cbp': r'\\b(?:cbp|customs and border protection)\\b'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate all dates in a given range, essentially given range makes a list of day dates\"\"\"\n",
    "def get_dates_in_range(start_date, end_date):\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    date_list = []\n",
    "    current = start\n",
    "    while current <= end:\n",
    "        date_list.append(current.strftime(\"%Y-%m-%d\"))\n",
    "        current += timedelta(days=1)\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to get Congressional Record data using the GovInfo API (this worked thankfully)\n",
    "Args:\n",
    "    date (str): Date in YYYY-MM-DD format\n",
    "Returns:\n",
    "    bool: Success status\n",
    "\"\"\"\n",
    "def get_congressional_record(date):\n",
    "   \n",
    "    package_id = f\"CREC-{date}\"\n",
    "    \n",
    "    # first check if the package exists for this date \n",
    "    package_url = f\"https://api.govinfo.gov/packages/{package_id}/summary\"\n",
    "    \n",
    "    params = {\n",
    "        'api_key': API_KEY\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # check if the package exists\n",
    "        response = requests.get(package_url, params=params)\n",
    "        \n",
    "        # if package doesn't exist or other error\n",
    "        if response.status_code != 200:\n",
    "            print(f\"No Congressional Record available for {date} (Status: {response.status_code})\")\n",
    "            return False\n",
    "        \n",
    "        # save the package summary\n",
    "        with open(f\"data/congressional_record/{package_id}-summary.json\", 'w') as f:\n",
    "            json.dump(response.json(), f)\n",
    "        \n",
    "        # get granules (speeches and entries) \n",
    "        granules_url = f\"https://api.govinfo.gov/packages/{package_id}/granules\"\n",
    "        granules_params = {\n",
    "            'api_key': API_KEY,\n",
    "            'offset': 0,\n",
    "            'pageSize': 100  # Max page size\n",
    "        }\n",
    "        \n",
    "        # get first page of granules\n",
    "        granules_response = requests.get(granules_url, params=granules_params)\n",
    "        \n",
    "        if granules_response.status_code != 200:\n",
    "            print(f\"Failed to get granules for {date} (Status: {granules_response.status_code})\")\n",
    "            return False\n",
    "            \n",
    "        # save the granules list\n",
    "        with open(f\"data/congressional_record/{package_id}-granules.json\", 'w') as f:\n",
    "            json.dump(granules_response.json(), f)\n",
    "            \n",
    "        # download content for each granule\n",
    "        granules = granules_response.json().get('granules', [])\n",
    "        \n",
    "        for granule in granules:\n",
    "            granule_id = granule.get('granuleId')\n",
    "            \n",
    "            # skip if no granule ID\n",
    "            if not granule_id:\n",
    "                continue\n",
    "            \n",
    "            # get the HTML content\n",
    "            content_url = f\"https://api.govinfo.gov/packages/{package_id}/granules/{granule_id}/htm\"\n",
    "            content_response = requests.get(content_url, params=params)\n",
    "            \n",
    "            if content_response.status_code == 200:\n",
    "                # save the HTML content\n",
    "                with open(f\"data/congressional_record/{package_id}-{granule_id}.html\", 'w', encoding='utf-8') as f:\n",
    "                    f.write(content_response.text)\n",
    "            \n",
    "            # respect rate limits\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        print(f\"Successfully downloaded Congressional Record for {date} ({len(granules)} granules)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data for {date}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API key with GovInfo API...\n",
      "✅ Success! Your API key is valid for the GovInfo API.\n",
      "Status code: 200\n",
      "\n",
      "Available collections:\n",
      "- Congressional Bills\n",
      "- Congressional Bill Status\n",
      "- Congressional Bill Summaries\n",
      "- United States Budget\n",
      "- Congressional Calendars\n"
     ]
    }
   ],
   "source": [
    "# the following is just an api key verification to make sure it works\n",
    "import requests\n",
    "\n",
    "# The API key you provided\n",
    "API_KEY = \"qAyZUrTJs4fdKIPefgekdiMQrCchdt979fIo58M1\"\n",
    "\n",
    "# Test the API key with a simple request to the GovInfo API\n",
    "test_url = \"https://api.govinfo.gov/collections\"\n",
    "params = {\n",
    "    'api_key': API_KEY\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(\"Testing API key with GovInfo API...\")\n",
    "    response = requests.get(test_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ Success! Your API key is valid for the GovInfo API.\")\n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        \n",
    "        # Show the first few collections to confirm we got real data\n",
    "        collections = response.json().get('collections', [])\n",
    "        if collections:\n",
    "            print(\"\\nAvailable collections:\")\n",
    "            for collection in collections[:5]:\n",
    "                print(f\"- {collection.get('collectionName', 'Unknown')}\")\n",
    "    elif response.status_code == 401 or response.status_code == 403:\n",
    "        print(\"❌ Authentication failed. Your API key appears to be invalid.\")\n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Received unexpected status code: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error occurred while testing the API key: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will download Congressional Record data for 289 dates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfde0186e3694b5abb96ed9efb22e662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Congressional Record available for 2019-01-01 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-01-02 (72 granules)\n",
      "No Congressional Record available for 2019-01-03 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-01-04 (100 granules)\n",
      "No Congressional Record available for 2019-01-05 (Status: 404)\n",
      "No Congressional Record available for 2019-01-06 (Status: 404)\n",
      "No Congressional Record available for 2019-01-07 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-01-08 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-09 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-10 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-11 (100 granules)\n",
      "No Congressional Record available for 2019-01-12 (Status: 404)\n",
      "No Congressional Record available for 2019-01-13 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-01-14 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-15 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-16 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-17 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-18 (44 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-19 (21 granules)\n",
      "No Congressional Record available for 2019-01-20 (Status: 404)\n",
      "No Congressional Record available for 2019-01-21 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-01-22 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-23 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-24 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-25 (74 granules)\n",
      "No Congressional Record available for 2019-01-26 (Status: 404)\n",
      "No Congressional Record available for 2019-01-27 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-01-28 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-29 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-30 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-31 (83 granules)\n",
      "No Congressional Record available for 2018-12-15 (Status: 404)\n",
      "No Congressional Record available for 2018-12-16 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2018-12-17 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-12-18 (63 granules)\n",
      "Successfully downloaded Congressional Record for 2018-12-19 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-12-20 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-12-21 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-12-22 (76 granules)\n",
      "No Congressional Record available for 2018-12-23 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2018-12-24 (21 granules)\n",
      "No Congressional Record available for 2018-12-25 (Status: 404)\n",
      "No Congressional Record available for 2018-12-26 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2018-12-27 (34 granules)\n",
      "No Congressional Record available for 2018-12-28 (Status: 404)\n",
      "No Congressional Record available for 2018-12-29 (Status: 404)\n",
      "No Congressional Record available for 2018-12-30 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2018-12-31 (38 granules)\n",
      "Successfully downloaded Congressional Record for 2017-09-01 (42 granules)\n",
      "No Congressional Record available for 2017-09-02 (Status: 404)\n",
      "No Congressional Record available for 2017-09-03 (Status: 404)\n",
      "No Congressional Record available for 2017-09-04 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2017-09-05 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2017-09-06 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2017-09-07 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2017-09-08 (87 granules)\n",
      "No Congressional Record available for 2017-09-09 (Status: 404)\n",
      "No Congressional Record available for 2017-09-10 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2017-09-11 (100 granules)\n",
      "Error retrieving data for 2017-09-12: HTTPSConnectionPool(host='api.govinfo.gov', port=443): Read timed out. (read timeout=None)\n",
      "Error retrieving data for 2017-09-13: HTTPSConnectionPool(host='api.govinfo.gov', port=443): Read timed out. (read timeout=None)\n",
      "Error retrieving data for 2017-09-14: HTTPSConnectionPool(host='api.govinfo.gov', port=443): Read timed out. (read timeout=None)\n",
      "No Congressional Record available for 2017-09-15 (Status: 404)\n",
      "No Congressional Record available for 2017-09-16 (Status: 404)\n",
      "No Congressional Record available for 2017-09-17 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2017-09-18 (73 granules)\n",
      "Successfully downloaded Congressional Record for 2017-09-19 (72 granules)\n",
      "No Congressional Record available for 2017-09-20 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2017-09-21 (67 granules)\n",
      "No Congressional Record available for 2017-09-22 (Status: 404)\n",
      "No Congressional Record available for 2017-09-23 (Status: 404)\n",
      "No Congressional Record available for 2017-09-24 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2017-09-25 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2017-09-26 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2017-09-27 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2017-09-28 (100 granules)\n",
      "No Congressional Record available for 2017-09-29 (Status: 404)\n",
      "No Congressional Record available for 2017-09-30 (Status: 404)\n",
      "No Congressional Record available for 2017-10-01 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2017-10-02 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2017-10-03 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2017-10-04 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2017-10-05 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2017-10-06 (45 granules)\n",
      "No Congressional Record available for 2017-10-07 (Status: 404)\n",
      "No Congressional Record available for 2017-10-08 (Status: 404)\n",
      "No Congressional Record available for 2017-10-09 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2017-10-10 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2017-10-11 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2017-10-12 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2017-10-13 (9 granules)\n",
      "No Congressional Record available for 2017-10-14 (Status: 404)\n",
      "No Congressional Record available for 2017-10-15 (Status: 404)\n",
      "No Congressional Record available for 2018-01-15 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2018-01-16 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-01-17 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-01-18 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-01-19 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-01-20 (51 granules)\n",
      "Successfully downloaded Congressional Record for 2018-01-21 (57 granules)\n",
      "Successfully downloaded Congressional Record for 2018-01-22 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-01-23 (68 granules)\n",
      "Successfully downloaded Congressional Record for 2018-01-24 (59 granules)\n",
      "Successfully downloaded Congressional Record for 2018-01-25 (100 granules)\n",
      "No Congressional Record available for 2018-01-26 (Status: 404)\n",
      "No Congressional Record available for 2018-01-27 (Status: 404)\n",
      "No Congressional Record available for 2018-01-28 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2018-01-29 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-01-30 (100 granules)\n",
      "No Congressional Record available for 2018-01-31 (Status: 404)\n",
      "No Congressional Record available for 2018-02-01 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2018-02-02 (73 granules)\n",
      "No Congressional Record available for 2018-02-03 (Status: 404)\n",
      "No Congressional Record available for 2018-02-04 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2018-02-05 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-02-06 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-02-07 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-02-08 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-02-09 (26 granules)\n",
      "No Congressional Record available for 2018-02-10 (Status: 404)\n",
      "No Congressional Record available for 2018-02-11 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2018-02-12 (51 granules)\n",
      "Successfully downloaded Congressional Record for 2018-02-13 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-02-14 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2018-02-15 (100 granules)\n",
      "No Congressional Record available for 2019-03-01 (Status: 404)\n",
      "No Congressional Record available for 2019-03-02 (Status: 404)\n",
      "No Congressional Record available for 2019-03-03 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-03-04 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-03-05 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-03-06 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-03-07 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-03-08 (100 granules)\n",
      "No Congressional Record available for 2019-03-09 (Status: 404)\n",
      "No Congressional Record available for 2019-03-10 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-03-11 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-03-12 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-03-13 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-03-14 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-03-15 (9 granules)\n",
      "No Congressional Record available for 2019-03-16 (Status: 404)\n",
      "No Congressional Record available for 2019-03-17 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-03-18 (78 granules)\n",
      "Successfully downloaded Congressional Record for 2019-03-19 (9 granules)\n",
      "No Congressional Record available for 2019-03-20 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-03-21 (84 granules)\n",
      "No Congressional Record available for 2019-03-22 (Status: 404)\n",
      "No Congressional Record available for 2019-03-23 (Status: 404)\n",
      "No Congressional Record available for 2019-03-24 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-03-25 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-03-26 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-03-27 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-03-28 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-03-29 (43 granules)\n",
      "No Congressional Record available for 2019-03-30 (Status: 404)\n",
      "No Congressional Record available for 2019-03-31 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-04-01 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-04-02 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-04-03 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-04-04 (100 granules)\n",
      "No Congressional Record available for 2019-04-05 (Status: 404)\n",
      "No Congressional Record available for 2019-04-06 (Status: 404)\n",
      "No Congressional Record available for 2019-04-07 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-04-08 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-04-09 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-04-10 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-04-11 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-04-12 (97 granules)\n",
      "No Congressional Record available for 2019-04-13 (Status: 404)\n",
      "No Congressional Record available for 2019-04-14 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-04-15 (63 granules)\n",
      "Successfully downloaded Congressional Record for 2020-01-15 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-01-16 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-01-17 (48 granules)\n",
      "No Congressional Record available for 2020-01-18 (Status: 404)\n",
      "No Congressional Record available for 2020-01-19 (Status: 404)\n",
      "No Congressional Record available for 2020-01-20 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2020-01-21 (71 granules)\n",
      "Successfully downloaded Congressional Record for 2020-01-22 (21 granules)\n",
      "Successfully downloaded Congressional Record for 2020-01-23 (16 granules)\n",
      "Successfully downloaded Congressional Record for 2020-01-24 (95 granules)\n",
      "Successfully downloaded Congressional Record for 2020-01-25 (11 granules)\n",
      "No Congressional Record available for 2020-01-26 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2020-01-27 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-01-28 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-01-29 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-01-30 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-01-31 (15 granules)\n",
      "No Congressional Record available for 2020-02-01 (Status: 404)\n",
      "No Congressional Record available for 2020-02-02 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2020-02-03 (77 granules)\n",
      "Successfully downloaded Congressional Record for 2020-02-04 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-02-05 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-02-06 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-02-07 (96 granules)\n",
      "No Congressional Record available for 2020-02-08 (Status: 404)\n",
      "No Congressional Record available for 2020-02-09 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2020-02-10 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-02-11 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-02-12 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-02-13 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-02-14 (51 granules)\n",
      "No Congressional Record available for 2020-02-15 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2020-09-01 (80 granules)\n",
      "No Congressional Record available for 2020-09-02 (Status: 404)\n",
      "No Congressional Record available for 2020-09-03 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2020-09-04 (69 granules)\n",
      "No Congressional Record available for 2020-09-05 (Status: 404)\n",
      "No Congressional Record available for 2020-09-06 (Status: 404)\n",
      "No Congressional Record available for 2020-09-07 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2020-09-08 (84 granules)\n",
      "Successfully downloaded Congressional Record for 2020-09-09 (60 granules)\n",
      "Successfully downloaded Congressional Record for 2020-09-10 (83 granules)\n",
      "Successfully downloaded Congressional Record for 2020-09-11 (98 granules)\n",
      "No Congressional Record available for 2020-09-12 (Status: 404)\n",
      "No Congressional Record available for 2020-09-13 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2020-09-14 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-09-15 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-09-16 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-09-17 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-09-18 (32 granules)\n",
      "No Congressional Record available for 2020-09-19 (Status: 404)\n",
      "No Congressional Record available for 2020-09-20 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2020-09-21 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-09-22 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-09-23 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-09-24 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-09-25 (25 granules)\n",
      "No Congressional Record available for 2020-09-26 (Status: 404)\n",
      "No Congressional Record available for 2020-09-27 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2020-09-28 (10 granules)\n",
      "Successfully downloaded Congressional Record for 2020-09-29 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-09-30 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-10-01 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2020-10-02 (100 granules)\n",
      "No Congressional Record available for 2020-10-03 (Status: 404)\n",
      "No Congressional Record available for 2020-10-04 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2020-10-05 (38 granules)\n",
      "Successfully downloaded Congressional Record for 2020-10-06 (56 granules)\n",
      "No Congressional Record available for 2020-10-07 (Status: 404)\n",
      "No Congressional Record available for 2020-10-08 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2020-10-09 (100 granules)\n",
      "No Congressional Record available for 2020-10-10 (Status: 404)\n",
      "No Congressional Record available for 2020-10-11 (Status: 404)\n",
      "No Congressional Record available for 2020-10-12 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2020-10-13 (65 granules)\n",
      "No Congressional Record available for 2020-10-14 (Status: 404)\n",
      "No Congressional Record available for 2020-10-15 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2021-01-20 (49 granules)\n",
      "Successfully downloaded Congressional Record for 2021-01-21 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2021-01-22 (49 granules)\n",
      "No Congressional Record available for 2021-01-23 (Status: 404)\n",
      "No Congressional Record available for 2021-01-24 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2021-01-25 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2021-01-26 (55 granules)\n",
      "Successfully downloaded Congressional Record for 2021-01-27 (53 granules)\n",
      "Successfully downloaded Congressional Record for 2021-01-28 (100 granules)\n",
      "No Congressional Record available for 2021-01-29 (Status: 404)\n",
      "No Congressional Record available for 2021-01-30 (Status: 404)\n",
      "No Congressional Record available for 2021-01-31 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2021-02-01 (87 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-02 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-03 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-04 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-05 (100 granules)\n",
      "No Congressional Record available for 2021-02-06 (Status: 404)\n",
      "No Congressional Record available for 2021-02-07 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2021-02-08 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-09 (17 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-10 (12 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-11 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-12 (39 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-13 (46 granules)\n",
      "No Congressional Record available for 2021-02-14 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2021-02-15 (82 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-16 (9 granules)\n",
      "No Congressional Record available for 2021-02-17 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2021-02-18 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-19 (9 granules)\n",
      "No Congressional Record available for 2021-02-20 (Status: 404)\n",
      "No Congressional Record available for 2021-02-21 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2021-02-22 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-23 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-24 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-25 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2021-02-26 (100 granules)\n",
      "No Congressional Record available for 2021-02-27 (Status: 404)\n",
      "No Congressional Record available for 2021-02-28 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2021-03-01 (100 granules)\n",
      "\n",
      "Data collection complete!\n",
      "Successfully downloaded data for 177 out of 289 dates\n",
      "Data saved to: data/congressional_record/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main function to download Congressional Record data\n",
    "\"\"\"\n",
    "def main():\n",
    "   \n",
    "    all_dates = []\n",
    "    \n",
    "    # generate all dates in the specified ranges\n",
    "    for start_date, end_date in date_ranges:\n",
    "        dates = get_dates_in_range(start_date, end_date)\n",
    "        all_dates.extend(dates)\n",
    "    \n",
    "    print(f\"Will download Congressional Record data for {len(all_dates)} dates\")\n",
    "    \n",
    "    # download data for each date\n",
    "    successful_downloads = 0\n",
    "    for date in tqdm(all_dates):\n",
    "        success = get_congressional_record(date)\n",
    "        if success:\n",
    "            successful_downloads += 1\n",
    "        \n",
    "        # wait between requests to avoid rate limiting\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"\\nData collection complete!\")\n",
    "    print(f\"Successfully downloaded data for {successful_downloads} out of {len(all_dates)} dates\")\n",
    "    print(f\"Data saved to: data/congressional_record/\")\n",
    "    \n",
    "    # create a simple summary file with immigration keywords to help with later analysis\n",
    "    with open('data/immigration_keywords.txt', 'w') as f:\n",
    "        keywords = [\n",
    "            'immigration', 'immigrant', 'migrant', 'migration', 'asylum', \n",
    "            'refugee', 'border', 'wall', 'undocumented', 'illegal alien',\n",
    "            'daca', 'dreamer', 'deportation', 'visa', 'citizenship',\n",
    "            'family separation', 'child detention', 'border security',\n",
    "            'border crisis', 'caravan', 'amnesty', 'path to citizenship'\n",
    "        ]\n",
    "        f.write('\\n'.join(keywords))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files with immigration content\n"
     ]
    }
   ],
   "source": [
    "# I just wanted to see if there were actually any immigrant related words in the data I collected so far, thankfully there was so now I want to convert these html files into\n",
    "# csv/excel files to have it more localized\n",
    "path = r\"C:\\Users\\kevin barcenas\\Documents\\GitHub\\LIN350Project\\congressional_record\"\n",
    "html_files = glob.glob(os.path.join(path, \"*.html\"))\n",
    "\n",
    "# Search ALL files (this might take a few minutes)\n",
    "immigration_files = []\n",
    "for file in html_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            content = f.read().lower()\n",
    "            found_terms = [term for term in immigration_terms if term in content]\n",
    "            if found_terms:\n",
    "                immigration_files.append({\n",
    "                    'file': file,\n",
    "                    'terms': found_terms\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "print(f\"Found {len(immigration_files)} files with immigration content\")\n",
    "\n",
    "# If we found any, save the list for reference\n",
    "if immigration_files:\n",
    "    df = pd.DataFrame(immigration_files)\n",
    "    df.to_csv(os.path.join(path, \"immigration_files.csv\"), index=False)\n",
    "    print(\"First 5 files with immigration content:\")\n",
    "    for item in immigration_files[:5]:\n",
    "        print(f\"  - {os.path.basename(item['file'])}: {', '.join(item['terms'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Input directory: C:\\Users\\kevin barcenas\\Documents\\GitHub\\LIN350Project\\congressional_record\\congressional_record\n",
      "Output directory: C:\\Users\\kevin barcenas\\Documents\\GitHub\\LIN350Project\\processed_data\n"
     ]
    }
   ],
   "source": [
    "# directory setup\n",
    "data_dir = r\"C:\\Users\\kevin barcenas\\Documents\\GitHub\\LIN350Project\\congressional_record\\congressional_record\"\n",
    "output_dir = r\"C:\\Users\\kevin barcenas\\Documents\\GitHub\\LIN350Project\\processed_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Input directory: {data_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14629 HTML files in C:\\Users\\kevin barcenas\\Documents\\GitHub\\LIN350Project\\congressional_record\\congressional_record\n",
      "\n",
      "Sample filenames:\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-2.html\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-3.html\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-4.html\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-5.html\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-6.html\n",
      "\n",
      "Successfully read first file. First 200 characters:\n",
      "<html> <head> <title>Congressional Record, Volume 163 Issue 141 (Friday, September 1, 2017)</title> </head> <body><pre> [Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)] [Daily\n"
     ]
    }
   ],
   "source": [
    "# get list of all HTML files\n",
    "html_files = glob.glob(os.path.join(data_dir, \"*.html\"))\n",
    "total_files = len(html_files)\n",
    "\n",
    "print(f\"Found {total_files} HTML files in {data_dir}\")\n",
    "\n",
    "# check the first few files to make sure we can access them\n",
    "if total_files > 0:\n",
    "    print(\"\\nSample filenames:\")\n",
    "    for file in html_files[:5]:\n",
    "        print(f\"  - {os.path.basename(file)}\")\n",
    "    \n",
    "    # try to open one file to verify access\n",
    "    try:\n",
    "        with open(html_files[0], 'r', encoding='utf-8') as f:\n",
    "            first_chars = f.read(200)\n",
    "        print(\"\\nSuccessfully read first file. First 200 characters:\")\n",
    "        print(first_chars.replace('\\n', ' ')[:200])\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError reading file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching 14629 files for immigration content...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 14629/14629 [00:30<00:00, 485.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1785 files with immigration content\n",
      "List saved to: C:\\Users\\kevin barcenas\\Documents\\GitHub\\LIN350Project\\processed_data\\immigration_files.csv\n",
      "\n",
      "Sample immigration-related files:\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-6.html: visa\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgE1151-4.html: refugee\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgE1152-3.html: immigration, immigrant, migrant, citizenship, deportation, undocumented, daca, dreamer, visa\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgE1154-4.html: undocumented, mexico\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgH6632-6.html: mexico\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# search for immigration-related content\n",
    "immigration_files = []\n",
    "print(f\"Searching {total_files} files for immigration content...\")\n",
    "\n",
    "for file in tqdm(html_files):\n",
    "    try:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().lower()\n",
    "            \n",
    "        # Check each term with its specific regex pattern\n",
    "        found_terms = []\n",
    "        for term, pattern in immigration_terms.items():\n",
    "            if re.search(pattern, content):\n",
    "                found_terms.append(term)\n",
    "        \n",
    "        if found_terms:\n",
    "            # Extract date from filename\n",
    "            filename = os.path.basename(file)\n",
    "            date_parts = filename.split('-')\n",
    "            if len(date_parts) >= 2:\n",
    "                date = date_parts[1]\n",
    "            else:\n",
    "                date = \"Unknown\"\n",
    "            \n",
    "            immigration_files.append({\n",
    "                'file': file,\n",
    "                'date': date,\n",
    "                'terms': ', '.join(found_terms)  # Convert list to string\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {os.path.basename(file)}: {e}\")\n",
    "\n",
    "# Save results to CSV\n",
    "if immigration_files:\n",
    "    immigration_df = pd.DataFrame(immigration_files)\n",
    "    csv_path = os.path.join(output_dir, \"immigration_files.csv\")\n",
    "    immigration_df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nFound {len(immigration_files)} files with immigration content\")\n",
    "    print(f\"List saved to: {csv_path}\")\n",
    "    \n",
    "    # Show sample of found files\n",
    "    print(\"\\nSample immigration-related files:\")\n",
    "    for file_info in immigration_files[:5]:\n",
    "        print(f\"  - {os.path.basename(file_info['file'])}: {file_info['terms']}\")\n",
    "else:\n",
    "    print(\"No immigration-related files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1785 immigration-related files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1785 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1785/1785 [00:02<00:00, 723.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully parsed 1785 files\n",
      "Data saved to: C:\\Users\\kevin barcenas\\Documents\\GitHub\\LIN350Project\\processed_data\\immigration_speeches.csv\n",
      "\n",
      "Top 10 speakers in the dataset:\n",
      "speaker_last\n",
      "Unknown       510\n",
      "RESOLUTION     79\n",
      "SCHUMER        70\n",
      "SA             57\n",
      "CORNYN         40\n",
      "DURBIN         40\n",
      "EC             28\n",
      "FEINSTEIN      25\n",
      "THUNE          24\n",
      "GRASSLEY       23\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Example of parsed data (first record):\n",
      "file_id: CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-6\n",
      "date: Friday, September 1, 2017\n",
      "chamber: House\n",
      "speaker_full: Unknown\n",
      "speaker_last: Unknown\n",
      "title: HOUSE\n",
      "full_text: \n",
      "[Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)]\n",
      "[Daily Digest]\n",
      "[Pages D909-D910]\n",
      "From the Congressional Record Online through the Government Publishing Office [www.gpo.gov]\n",
      "...\n",
      "immigration_terms: visa\n"
     ]
    }
   ],
   "source": [
    "immigration_files_csv = os.path.join(output_dir, \"immigration_files.csv\")\n",
    "# Load the list of immigration-related files\n",
    "if not os.path.exists(immigration_files_csv):\n",
    "    print(f\"Error: Immigration files list not found at {immigration_files_csv}\")\n",
    "    exit()\n",
    "\n",
    "immigration_df = pd.read_csv(immigration_files_csv)\n",
    "print(f\"Processing {len(immigration_df)} immigration-related files...\")\n",
    "\n",
    "# Function to extract structured data from HTML\n",
    "def parse_congressional_record(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        pre_content = soup.find('pre')\n",
    "        \n",
    "        if not pre_content:\n",
    "            return None\n",
    "        \n",
    "        text = pre_content.get_text()\n",
    "        \n",
    "        # Extract date from header\n",
    "        date_match = re.search(r'\\[Congressional Record Volume \\d+, Number \\d+ \\(([^)]+)\\)\\]', text)\n",
    "        date = date_match.group(1) if date_match else \"Unknown\"\n",
    "        \n",
    "        # Extract chamber (Senate or House)\n",
    "        chamber = \"Unknown\"\n",
    "        if \"Senate\" in text[:500]:\n",
    "            chamber = \"Senate\"\n",
    "        elif \"House\" in text[:500]:\n",
    "            chamber = \"House\"\n",
    "        \n",
    "        # Extract speaker - look for common patterns\n",
    "        speaker_patterns = [\n",
    "            # Pattern for \"Mr. SMITH\" format\n",
    "            r'(?:^|\\n)\\s{1,10}((?:Mr\\.|Mrs\\.|Ms\\.|Senator|Representative)\\s+([A-Z]+))\\.', \n",
    "            # Pattern for \"The CHAIRMAN\" format\n",
    "            r'(?:^|\\n)\\s{1,10}(The\\s+([A-Z]+))\\.', \n",
    "            # Fallback pattern for other formats\n",
    "            r'(?:^|\\n)\\s{1,10}([A-Z]{2,}(?:\\s+[A-Z]+)*)\\.?' \n",
    "        ]\n",
    "        \n",
    "        speaker_full = \"Unknown\"\n",
    "        speaker_last = \"Unknown\"\n",
    "        \n",
    "        for pattern in speaker_patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                speaker_full = match.group(1)\n",
    "                if len(match.groups()) > 1 and match.group(2):\n",
    "                    speaker_last = match.group(2)\n",
    "                else:\n",
    "                    # Extract last name from full name\n",
    "                    parts = speaker_full.split()\n",
    "                    if parts:\n",
    "                        speaker_last = parts[-1]\n",
    "                break\n",
    "        \n",
    "        # Extract title - look for all-caps line after header\n",
    "        title_match = re.search(r'\\n\\s*([A-Z][A-Z\\s\\'\\\",.()-]+?)\\s*\\n', text)\n",
    "        title = title_match.group(1).strip() if title_match else \"Unknown\"\n",
    "        \n",
    "        # Get granule ID from filename\n",
    "        filename = os.path.basename(file_path)\n",
    "        granule_id = filename.replace(\".html\", \"\")\n",
    "        \n",
    "        return {\n",
    "            'file_id': granule_id,\n",
    "            'date': date,\n",
    "            'chamber': chamber,\n",
    "            'speaker_full': speaker_full,\n",
    "            'speaker_last': speaker_last,\n",
    "            'title': title,\n",
    "            'full_text': text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {os.path.basename(file_path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process each file in the immigration list\n",
    "parsed_data = []\n",
    "for _, row in tqdm(immigration_df.iterrows(), total=len(immigration_df)):\n",
    "    file_path = row['file']\n",
    "    extracted_data = parse_congressional_record(file_path)\n",
    "    \n",
    "    if extracted_data:\n",
    "        # Add the immigration terms found\n",
    "        extracted_data['immigration_terms'] = row['terms']\n",
    "        parsed_data.append(extracted_data)\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "if parsed_data:\n",
    "    parsed_df = pd.DataFrame(parsed_data)\n",
    "    csv_path = os.path.join(output_dir, \"immigration_speeches.csv\")\n",
    "    parsed_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\nSuccessfully parsed {len(parsed_data)} files\")\n",
    "    print(f\"Data saved to: {csv_path}\")\n",
    "    \n",
    "    # Print summary of speakers found\n",
    "    speaker_counts = parsed_df['speaker_last'].value_counts()\n",
    "    print(f\"\\nTop 10 speakers in the dataset:\")\n",
    "    print(speaker_counts.head(10))\n",
    "    \n",
    "    # Print example of first record\n",
    "    print(\"\\nExample of parsed data (first record):\")\n",
    "    for key, value in parsed_data[0].items():\n",
    "        if key == 'full_text':\n",
    "            print(f\"{key}: {value[:200]}...\") # Print only first 200 chars of text\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"No data could be parsed from the files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1785 records from original CSV\n",
      "\n",
      "Created cleaned dataset with 1785 records\n",
      "Created filtered dataset with 1094 actual speeches\n",
      "\n",
      "Party distribution in speeches:\n",
      "party\n",
      "D    147\n",
      "R     92\n",
      "I     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Speech category distribution:\n",
      "speech_category\n",
      "children           406\n",
      "other              283\n",
      "border_security    181\n",
      "general             92\n",
      "legal_status        89\n",
      "asylum              43\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "speeches_csv = os.path.join(output_dir, \"immigration_speeches.csv\")\n",
    "\n",
    "# Load the existing data\n",
    "speeches_df = pd.read_csv(speeches_csv)\n",
    "print(f\"Loaded {len(speeches_df)} records from original CSV\")\n",
    "\n",
    "# Function to clean and improve data\n",
    "def clean_data(df):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # 1. Convert dates to standard format\n",
    "    def standardize_date(date_str):\n",
    "        try:\n",
    "            if pd.isna(date_str) or date_str == \"Unknown\":\n",
    "                return None\n",
    "            # Parse date string to datetime object\n",
    "            date_obj = datetime.strptime(date_str, \"%A, %B %d, %Y\")\n",
    "            # Convert to standard format\n",
    "            return date_obj.strftime(\"%Y-%m-%d\")\n",
    "        except:\n",
    "            return date_str\n",
    "    \n",
    "    cleaned_df['date_standard'] = cleaned_df['date'].apply(standardize_date)\n",
    "    \n",
    "    # 2. Identify real speeches vs. procedural text\n",
    "    def is_real_speech(row):\n",
    "        # Check if it's likely a speech by a member of Congress\n",
    "        \n",
    "        # If speaker is Unknown, probably not a speech\n",
    "        if row['speaker_last'] == \"Unknown\":\n",
    "            return False\n",
    "        \n",
    "        # Check for procedural titles\n",
    "        procedural_titles = ['HOUSE', 'SENATE', 'PRAYER', 'PLEDGE', 'ADJOURNMENT', \n",
    "                            'RECESS', 'AMENDMENT', 'RECORD', 'MOTION', 'RESOLUTION']\n",
    "        if any(title in row['title'] for title in procedural_titles):\n",
    "            return False\n",
    "        \n",
    "        # Check for very short texts (likely not speeches)\n",
    "        if len(row['full_text']) < 500:\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    cleaned_df['is_speech'] = cleaned_df.apply(is_real_speech, axis=1)\n",
    "    \n",
    "    # 3. Categorize speech type\n",
    "    def categorize_speech(row):\n",
    "        text = row['full_text'].lower()\n",
    "        \n",
    "        if not row['is_speech']:\n",
    "            return \"procedural\"\n",
    "            \n",
    "        categories = {\n",
    "            \"border_security\": [\"border security\", \"border wall\", \"border crisis\"],\n",
    "            \"legal_status\": [\"undocumented\", \"illegal alien\", \"unauthorized\", \"amnesty\", \"path to citizenship\"],\n",
    "            \"children\": [\"daca\", \"dreamer\", \"child\", \"family separation\"],\n",
    "            \"asylum\": [\"asylum\", \"refugee\", \"humanitarian\"],\n",
    "            \"general\": [\"immigration\", \"immigrant\", \"migrant\"]\n",
    "        }\n",
    "        \n",
    "        for category, terms in categories.items():\n",
    "            if any(term in text for term in terms):\n",
    "                return category\n",
    "                \n",
    "        return \"other\"\n",
    "    \n",
    "    cleaned_df['speech_category'] = cleaned_df.apply(categorize_speech, axis=1)\n",
    "    \n",
    "    # 4. Extract a summary from the full text (first 300 characters)\n",
    "    def extract_summary(text):\n",
    "        # Remove header content in square brackets\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        # Remove whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # Take first 300 characters\n",
    "        return text[:300] + \"...\" if len(text) > 300 else text\n",
    "    \n",
    "    cleaned_df['speech_summary'] = cleaned_df['full_text'].apply(extract_summary)\n",
    "    \n",
    "    # 5. Add party information for key members\n",
    "    party_lookup = {\n",
    "        'MCCONNELL': 'R', 'SCHUMER': 'D', 'DURBIN': 'D', 'THUNE': 'R', 'CORNYN': 'R',\n",
    "        'GRASSLEY': 'R', 'FEINSTEIN': 'D', 'LEAHY': 'D', 'CRUZ': 'R', 'GRAHAM': 'R',\n",
    "        'PELOSI': 'D', 'MCCARTHY': 'R', 'HOYER': 'D', 'SCALISE': 'R', 'CLYBURN': 'D',\n",
    "        'HARRIS': 'D', 'WARREN': 'D', 'SANDERS': 'I', 'RUBIO': 'R', 'COTTON': 'R'\n",
    "    }\n",
    "    \n",
    "    cleaned_df['party'] = cleaned_df['speaker_last'].map(party_lookup)\n",
    "    \n",
    "    # 6. Keep only relevant columns in a useful order\n",
    "    columns_order = [\n",
    "        'file_id', 'date_standard', 'chamber', 'speaker_full', 'speaker_last', \n",
    "        'party', 'title', 'is_speech', 'speech_category', 'speech_summary', \n",
    "        'immigration_terms', 'full_text'\n",
    "    ]\n",
    "    \n",
    "    # Return only the columns we want\n",
    "    return cleaned_df[columns_order]\n",
    "\n",
    "# Clean the data\n",
    "cleaned_speeches = clean_data(speeches_df)\n",
    "\n",
    "# Create filtered dataset with only actual speeches\n",
    "speeches_only = cleaned_speeches[cleaned_speeches['is_speech'] == True]\n",
    "\n",
    "# Save the cleaned and filtered data\n",
    "cleaned_speeches.to_csv(os.path.join(output_dir, \"immigration_data_clean.csv\"), index=False)\n",
    "speeches_only.to_csv(os.path.join(output_dir, \"immigration_speeches_clean.csv\"), index=False)\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nCreated cleaned dataset with {len(cleaned_speeches)} records\")\n",
    "print(f\"Created filtered dataset with {len(speeches_only)} actual speeches\")\n",
    "\n",
    "# Show party distribution \n",
    "if len(speeches_only) > 0:\n",
    "    party_counts = speeches_only['party'].value_counts()\n",
    "    print(\"\\nParty distribution in speeches:\")\n",
    "    print(party_counts)\n",
    "\n",
    "# Show category distribution\n",
    "category_counts = speeches_only['speech_category'].value_counts()\n",
    "print(\"\\nSpeech category distribution:\")\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1094 records from clean speeches CSV\n",
      "Available columns in the dataset:\n",
      "- file_id\n",
      "- date_standard\n",
      "- chamber\n",
      "- speaker_full\n",
      "- speaker_last\n",
      "- party\n",
      "- title\n",
      "- is_speech\n",
      "- speech_category\n",
      "- speech_summary\n",
      "- immigration_terms\n",
      "- full_text\n",
      "Creating text excerpts...\n",
      "Creating sample text files...\n",
      "\n",
      "Created Excel-friendly version: C:\\Users\\kevin barcenas\\Documents\\GitHub\\LIN350Project\\processed_data\\immigration_data_excel.xlsx\n",
      "Created compact CSV without full text: C:\\Users\\kevin barcenas\\Documents\\GitHub\\LIN350Project\\processed_data\\immigration_data_compact.csv\n",
      "Created sample text files in: C:\\Users\\kevin barcenas\\Documents\\GitHub\\LIN350Project\\processed_data\\speech_samples\n",
      "\n",
      "Summary statistics:\n",
      "\n",
      "Speeches by party:\n",
      "party\n",
      "D    147\n",
      "R     92\n",
      "I     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Speeches by category:\n",
      "speech_category\n",
      "children           406\n",
      "other              283\n",
      "border_security    181\n",
      "general             92\n",
      "legal_status        89\n",
      "asylum              43\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Directory paths\n",
    "base_dir = r\"C:\\Users\\kevin barcenas\\Documents\\GitHub\\LIN350Project\"\n",
    "output_dir = os.path.join(base_dir, \"processed_data\")\n",
    "speeches_csv = os.path.join(output_dir, \"immigration_speeches_clean.csv\")\n",
    "\n",
    "# Load the existing data\n",
    "speeches_df = pd.read_csv(speeches_csv)\n",
    "print(f\"Loaded {len(speeches_df)} records from clean speeches CSV\")\n",
    "\n",
    "# Check available columns\n",
    "print(\"Available columns in the dataset:\")\n",
    "for col in speeches_df.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "# Create a copy to avoid modifying the original\n",
    "df_clean = speeches_df.copy()\n",
    "\n",
    "# Add party information if 'party' column doesn't exist but 'speaker_last' does\n",
    "if 'speaker_last' in df_clean.columns and 'party' not in df_clean.columns:\n",
    "    print(\"\\nAdding party information...\")\n",
    "    \n",
    "    party_lookup = {\n",
    "        # Senate leadership\n",
    "        'MCCONNELL': 'R', 'SCHUMER': 'D', 'DURBIN': 'D', 'THUNE': 'R', 'CORNYN': 'R',\n",
    "        'GRASSLEY': 'R', 'FEINSTEIN': 'D', 'LEAHY': 'D', 'CRUZ': 'R', 'GRAHAM': 'R',\n",
    "        'SANDERS': 'I', 'HARRIS': 'D', 'WARREN': 'D', 'RUBIO': 'R', 'COTTON': 'R',\n",
    "        # House leadership\n",
    "        'PELOSI': 'D', 'MCCARTHY': 'R', 'HOYER': 'D', 'SCALISE': 'R', 'CLYBURN': 'D',\n",
    "        # Add more as needed\n",
    "        'CAPUANO': 'D', 'MATSUI': 'D', 'HULTGREN': 'R', 'KRISHNAMOORTHI': 'D',\n",
    "        'PAYNE': 'D', 'LANGEVIN': 'D', 'BLUMENAUER': 'D', 'ROHRABACHER': 'R'\n",
    "    }\n",
    "    \n",
    "    df_clean['party'] = df_clean['speaker_last'].map(party_lookup)\n",
    "    df_clean['party'] = df_clean['party'].fillna('Unknown')\n",
    "\n",
    "# Add speech categorization if needed\n",
    "if 'full_text' in df_clean.columns and 'speech_category' not in df_clean.columns:\n",
    "    print(\"Adding speech categorization...\")\n",
    "    \n",
    "    def categorize_speech(text):\n",
    "        if pd.isna(text):\n",
    "            return \"unknown\"\n",
    "            \n",
    "        text = str(text).lower()\n",
    "        \n",
    "        if any(term in text for term in [\"border security\", \"border wall\", \"border crisis\", \"wall\"]):\n",
    "            return \"border_security\"\n",
    "        elif any(term in text for term in [\"undocumented\", \"illegal alien\", \"unauthorized\", \"amnesty\", \"path to citizenship\"]):\n",
    "            return \"legal_status\"\n",
    "        elif any(term in text for term in [\"daca\", \"dreamer\", \"child\", \"family separation\"]):\n",
    "            return \"children\"\n",
    "        elif any(term in text for term in [\"asylum\", \"refugee\"]):\n",
    "            return \"asylum\"\n",
    "        elif any(term in text for term in [\"immigration\", \"immigrant\", \"migrant\"]):\n",
    "            return \"general\"\n",
    "        else:\n",
    "            return \"other\"\n",
    "    \n",
    "    df_clean['speech_category'] = df_clean['full_text'].apply(categorize_speech)\n",
    "\n",
    "# Ensure 'date_standard' column exists (standardized date format)\n",
    "if 'date' in df_clean.columns and 'date_standard' not in df_clean.columns:\n",
    "    print(\"Standardizing dates...\")\n",
    "    \n",
    "    def standardize_date(date_str):\n",
    "        try:\n",
    "            if pd.isna(date_str) or date_str == \"Unknown\":\n",
    "                return None\n",
    "                \n",
    "            # Handle different date formats\n",
    "            formats = [\"%A, %B %d, %Y\", \"%Y-%m-%d\", \"%m/%d/%Y\"]\n",
    "            \n",
    "            for fmt in formats:\n",
    "                try:\n",
    "                    date_obj = pd.to_datetime(date_str, format=fmt)\n",
    "                    return date_obj.strftime(\"%Y-%m-%d\")\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "            return date_str\n",
    "        except:\n",
    "            return date_str\n",
    "    \n",
    "    df_clean['date_standard'] = df_clean['date'].apply(standardize_date)\n",
    "\n",
    "# Add text excerpt for Excel\n",
    "if 'full_text' in df_clean.columns:\n",
    "    print(\"Creating text excerpts...\")\n",
    "    df_clean['text_excerpt'] = df_clean['full_text'].apply(\n",
    "        lambda x: (str(x)[:500] + '...') if isinstance(x, str) and len(str(x)) > 500 else x\n",
    "    )\n",
    "\n",
    "# Add text length column\n",
    "if 'full_text' in df_clean.columns:\n",
    "    df_clean['text_length'] = df_clean['full_text'].apply(\n",
    "        lambda x: len(str(x)) if not pd.isna(x) else 0\n",
    "    )\n",
    "\n",
    "# Create compact version (no full text)\n",
    "compact_df = df_clean.copy()\n",
    "if 'full_text' in compact_df.columns:\n",
    "    compact_df = compact_df.drop('full_text', axis=1)\n",
    "\n",
    "# Create Excel version (no full text, with excerpt)\n",
    "excel_df = df_clean.copy()\n",
    "if 'full_text' in excel_df.columns:\n",
    "    excel_df = excel_df.drop('full_text', axis=1)\n",
    "\n",
    "# Save the compact version\n",
    "compact_csv = os.path.join(output_dir, \"immigration_data_compact.csv\")\n",
    "compact_df.to_csv(compact_csv, index=False)\n",
    "\n",
    "# Save the Excel-friendly version\n",
    "excel_file = os.path.join(output_dir, \"immigration_data_excel.xlsx\")\n",
    "with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:\n",
    "    excel_df.to_excel(writer, sheet_name='Speeches', index=False)\n",
    "    \n",
    "    # Add some formatting\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['Speeches']\n",
    "    \n",
    "    # Set default column widths\n",
    "    for col_num, _ in enumerate(excel_df.columns):\n",
    "        worksheet.set_column(col_num, col_num, 15)\n",
    "    \n",
    "    # Set specific columns wider if they exist\n",
    "    col_widths = {\n",
    "        'file_id': 30,\n",
    "        'full_text': 70,\n",
    "        'text_excerpt': 70,\n",
    "        'title': 30\n",
    "    }\n",
    "    \n",
    "    for col_name, width in col_widths.items():\n",
    "        if col_name in excel_df.columns:\n",
    "            col_idx = excel_df.columns.get_loc(col_name)\n",
    "            worksheet.set_column(col_idx, col_idx, width)\n",
    "\n",
    "# Create sample text files\n",
    "text_dir = os.path.join(output_dir, \"speech_samples\")\n",
    "os.makedirs(text_dir, exist_ok=True)\n",
    "\n",
    "if 'full_text' in df_clean.columns:\n",
    "    print(\"Creating sample text files...\")\n",
    "    \n",
    "    # Get sample of speeches (max 20)\n",
    "    sample_size = min(20, len(df_clean))\n",
    "    for idx, row in df_clean.head(sample_size).iterrows():\n",
    "        # Create a clean filename\n",
    "        if 'speaker_last' in row and not pd.isna(row['speaker_last']):\n",
    "            speaker = str(row['speaker_last'])\n",
    "        else:\n",
    "            speaker = \"Unknown\"\n",
    "            \n",
    "        clean_name = ''.join(c if c.isalnum() else '_' for c in speaker)\n",
    "        \n",
    "        # Add date to filename if available\n",
    "        date_str = \"\"\n",
    "        if 'date_standard' in row and not pd.isna(row['date_standard']):\n",
    "            date_str = str(row['date_standard'])\n",
    "        \n",
    "        file_name = f\"{clean_name}_{date_str}_{idx}.txt\"\n",
    "        file_path = os.path.join(text_dir, file_name)\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            # Write metadata\n",
    "            metadata_cols = ['speaker_full', 'speaker_last', 'party', 'date', 'title', 'chamber', 'speech_category']\n",
    "            for col in metadata_cols:\n",
    "                if col in row and not pd.isna(row[col]):\n",
    "                    f.write(f\"{col}: {row[col]}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "            \n",
    "            # Write full text\n",
    "            if 'full_text' in row and not pd.isna(row['full_text']):\n",
    "                f.write(str(row['full_text']))\n",
    "            else:\n",
    "                f.write(\"[No text available]\")\n",
    "            \n",
    "print(f\"\\nCreated Excel-friendly version: {excel_file}\")\n",
    "print(f\"Created compact CSV without full text: {compact_csv}\")\n",
    "print(f\"Created sample text files in: {text_dir}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "if 'party' in df_clean.columns:\n",
    "    party_counts = df_clean['party'].value_counts()\n",
    "    print(\"\\nSpeeches by party:\")\n",
    "    print(party_counts)\n",
    "\n",
    "if 'speech_category' in df_clean.columns:\n",
    "    category_counts = df_clean['speech_category'].value_counts()\n",
    "    print(\"\\nSpeeches by category:\")\n",
    "    print(category_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great now going forward I'll have these two files to work with\n",
    "\"processed_data/immigration_data_compact.csv\" and \"processed_data/immigration_speeches_clean.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)]\n",
      "[Extensions of Remarks]\n",
      "[Page E1151]\n",
      "From the Congressional Record Online through the Government Publishing Office [www.gpo.gov]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " HONORING THE 40TH ANNIVERSARY OF SERVICE BY REVEREND DR. AMOS C. BROWN\n",
      "\n",
      "                                 ______\n",
      "                                 \n",
      "\n",
      "                           HON. NANCY PELOSI\n",
      "\n",
      "                             of california\n",
      "\n",
      "                    in the house of representatives\n",
      "\n",
      "                       Friday, September 1, 2017\n",
      "\n",
      "  Ms. PELOSI. Mr. Speaker, I rise with pride today to join my beloved \n",
      "city in recognizing the 40th Anniversary of Reverend Dr. Amos C. Brown \n",
      "as the pastor of the Third Baptist Church of San Francisco and honoring \n",
      "his lifetime of moral leadership, political activism and spiritual \n",
      "guidance.\n",
      "  Born and raised in Jackson, Mississippi, Reverend Brown's history of \n",
      "activism and his deep involvement in the civil rights movement began at \n",
      "an early age. He was only a teenager when he assumed leadership of his \n",
      "local NAACP chapter and was driven cross-country to San Francisco by \n",
      "activist Medgar Evers to meet the Reverend Martin Luther King Jr. \n",
      "Later, Brown attended Morehouse College where he was hand-selected by \n",
      "King as one of his eight students, inspiring him to seek a lifetime of \n",
      "service on behalf of civil rights and social justice. He was arrested \n",
      "together with Dr. King during a lunch counter sit-in Atlanta department \n",
      "store, and became one of the fearless Freedom Riders in 1961.\n",
      "  Brown received both a Mastery of Divinity from Crozer Theological \n",
      "Seminary and a Doctor of Ministry from United Theological Seminary. Our \n",
      "city was fortunate to welcome the Pastor and his beloved wife Jane \n",
      "Smith Brown in 1976, when they made San Francisco their home and the \n",
      "Third Baptist Church their place of worship and leadership.\n",
      "  Brown's vision of the church as a holy place of prayer, and also a \n",
      "place of civic engagement and social action, transformed the Third \n",
      "Baptist Church into a congregation at the forefront of the fight for \n",
      "civil rights. His unwavering belief in future generations drove the \n",
      "church to establish a summer school program, an after-school academic \n",
      "enrichment program, ``Back on Track'', and the Charles A. Tindley \n",
      "Academy of Music.\n",
      "  Brown's influence also extended beyond his local community when the \n",
      "church sponsored 80 children from Tanzania to receive heart surgery in \n",
      "the United States, and his congregation sponsored more African refugees \n",
      "than any other local congregation in the nation. This global reach and \n",
      "visionary leadership was recognized when Brown was appointed as a \n",
      "delegate to the 2001 United Nations Conference on Race and Intolerance \n",
      "in Durban, South Africa, representing the National Board of the NAACP.\n",
      "  Closer to home, Brown was awarded the Martin Luther King Jr. \n",
      "Ministerial Award for outstanding leadership and contributions to the \n",
      "Black Church in America and he was inducted into the International Hall \n",
      "of Fame of the King International Chapel at Morehouse College. He has \n",
      "served as the president of the NAACP Branch in San Francisco, as a San \n",
      "Francisco Community College Trustee and as a San Francisco County \n",
      "Supervisor.\n",
      "  A passionate fighter and voice for the most vulnerable in our \n",
      "community, Reverend Amos Brown has served as a source of change, \n",
      "strength and faith for 40 years. I congratulate him, along with his \n",
      "wife Jane and his children Amos C. Brown Jr., David Josephus and Kizzie \n",
      "Marie, on this milestone celebration and extend my best wishes as he \n",
      "continues to lead and inspire our community.\n",
      "\n",
      "                          ____________________\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_df = pd.read_csv(\"processed_data/immigration_speeches_clean.csv\")\n",
    "full_text_df = full_df[\"full_text\"]\n",
    "print(full_text_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
