{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIN350 Course Project - The Language of Immigration Politics: Terminology Differences Across Party Lines in Congressional Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way I usually run jupyter notebooks is opening the anaconda prompt terminal and running the command *jupyter notebook* from there I go to visual studio and click on select kernel -> existing jupyter server -> localhost or you can copy and paste the url of the tab that opened up with the *jupyter notebook* command and then click on python and that should be it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep track of the work we're doing together we can use a github repository to update changes and sync up our work. The usual workflow for this should be.\n",
    "1. Any changes you have in your laptop can be added to the repository with \"git add ./\" from the terminal the notebook is in\n",
    "2. After adding the files and changes you can use \"git commit -m 'message here'\" For the message make sure its in quotations and it can be anything\n",
    "3. After adding and commiting you can \"git push\" which pushes ur changes to the repository\n",
    "4. Let's say there's changes in the repository that are not in your laptop you can fetch them with \"git pull\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other setup you might need to do is set environement variables in local computer since we don't want to share that in the repository for privacy issues. So to do this you would run commands in your notebook to set it up. I'll show you\n",
    "1. running \"%env\" in a code block will show you all the environment variables in the jupyter environment\n",
    "2. to set up the enviroment variable for our project run the command \"%env API_KEY=apikeyfromourgoogledocs\"\n",
    "3. After that running the first cell of code will setup the api key to be used as API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Record Data Collector - Very simple for now, simple text data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will download Congressional Record data for 31 dates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3a5d190ed24491bab561be09d143e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Congressional Record available for 2019-01-01 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-01-02 (72 granules)\n",
      "No Congressional Record available for 2019-01-03 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-01-04 (100 granules)\n",
      "No Congressional Record available for 2019-01-05 (Status: 404)\n",
      "No Congressional Record available for 2019-01-06 (Status: 404)\n",
      "No Congressional Record available for 2019-01-07 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-01-08 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-09 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-10 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-11 (100 granules)\n",
      "No Congressional Record available for 2019-01-12 (Status: 404)\n",
      "No Congressional Record available for 2019-01-13 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-01-14 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-15 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-16 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-17 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-18 (44 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-19 (21 granules)\n",
      "No Congressional Record available for 2019-01-20 (Status: 404)\n",
      "No Congressional Record available for 2019-01-21 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-01-22 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-23 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-24 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-25 (74 granules)\n",
      "No Congressional Record available for 2019-01-26 (Status: 404)\n",
      "No Congressional Record available for 2019-01-27 (Status: 404)\n",
      "Successfully downloaded Congressional Record for 2019-01-28 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-29 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-30 (100 granules)\n",
      "Successfully downloaded Congressional Record for 2019-01-31 (83 granules)\n",
      "\n",
      "Data collection complete!\n",
      "Successfully downloaded data for 20 out of 31 dates\n",
      "Data saved to: data/congressional_record/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# create directories for data storage\n",
    "os.makedirs('data/congressional_record', exist_ok=True)\n",
    "\n",
    "# set your API key here (get one from https://api.data.gov/signup/)\n",
    "API_KEY = os.environ[\"API_KEY\"]\n",
    "\n",
    "# Define date ranges for your study (immigration debates 2018-2023)\n",
    "date_ranges = [\n",
    "    # 2018 - Family separation policy debates -> for now commented out because I wanted to test collection only 2019 January data\n",
    "    #(\"2018-06-01\", \"2018-06-30\"),\n",
    "    # 2019 - Border wall government shutdown -> for now \n",
    "    (\"2019-01-01\", \"2019-01-31\"),\n",
    "    # 2020 - Selected periods\n",
    "    #(\"2020-02-01\", \"2020-02-15\"),\n",
    "    # 2021 - Biden immigration policy\n",
    "    #(\"2021-02-01\", \"2021-02-15\"),\n",
    "    # 2022 - Border discussions\n",
    "    #(\"2022-05-01\", \"2022-05-15\")\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate all dates in a given range, essentially given range makes a list of day dates\"\"\"\n",
    "def get_dates_in_range(start_date, end_date):\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    date_list = []\n",
    "    current = start\n",
    "    while current <= end:\n",
    "        date_list.append(current.strftime(\"%Y-%m-%d\"))\n",
    "        current += timedelta(days=1)\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to get Congressional Record data using the GovInfo API (this worked thankfully)\n",
    "Args:\n",
    "    date (str): Date in YYYY-MM-DD format\n",
    "Returns:\n",
    "    bool: Success status\n",
    "\"\"\"\n",
    "def get_congressional_record(date):\n",
    "   \n",
    "    package_id = f\"CREC-{date}\"\n",
    "    \n",
    "    # first check if the package exists for this date \n",
    "    package_url = f\"https://api.govinfo.gov/packages/{package_id}/summary\"\n",
    "    \n",
    "    params = {\n",
    "        'api_key': API_KEY\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # check if the package exists\n",
    "        response = requests.get(package_url, params=params)\n",
    "        \n",
    "        # if package doesn't exist or other error\n",
    "        if response.status_code != 200:\n",
    "            print(f\"No Congressional Record available for {date} (Status: {response.status_code})\")\n",
    "            return False\n",
    "        \n",
    "        # save the package summary\n",
    "        with open(f\"data/congressional_record/{package_id}-summary.json\", 'w') as f:\n",
    "            json.dump(response.json(), f)\n",
    "        \n",
    "        # get granules (speeches and entries) \n",
    "        granules_url = f\"https://api.govinfo.gov/packages/{package_id}/granules\"\n",
    "        granules_params = {\n",
    "            'api_key': API_KEY,\n",
    "            'offset': 0,\n",
    "            'pageSize': 100  # Max page size\n",
    "        }\n",
    "        \n",
    "        # get first page of granules\n",
    "        granules_response = requests.get(granules_url, params=granules_params)\n",
    "        \n",
    "        if granules_response.status_code != 200:\n",
    "            print(f\"Failed to get granules for {date} (Status: {granules_response.status_code})\")\n",
    "            return False\n",
    "            \n",
    "        # save the granules list\n",
    "        with open(f\"data/congressional_record/{package_id}-granules.json\", 'w') as f:\n",
    "            json.dump(granules_response.json(), f)\n",
    "            \n",
    "        # download content for each granule\n",
    "        granules = granules_response.json().get('granules', [])\n",
    "        \n",
    "        for granule in granules:\n",
    "            granule_id = granule.get('granuleId')\n",
    "            \n",
    "            # skip if no granule ID\n",
    "            if not granule_id:\n",
    "                continue\n",
    "            \n",
    "            # get the HTML content\n",
    "            content_url = f\"https://api.govinfo.gov/packages/{package_id}/granules/{granule_id}/htm\"\n",
    "            content_response = requests.get(content_url, params=params)\n",
    "            \n",
    "            if content_response.status_code == 200:\n",
    "                # save the HTML content\n",
    "                with open(f\"data/congressional_record/{package_id}-{granule_id}.html\", 'w', encoding='utf-8') as f:\n",
    "                    f.write(content_response.text)\n",
    "            \n",
    "            # respect rate limits\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        print(f\"Successfully downloaded Congressional Record for {date} ({len(granules)} granules)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data for {date}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function to download Congressional Record data\n",
    "\"\"\"\n",
    "def main():\n",
    "   \n",
    "    all_dates = []\n",
    "    \n",
    "    # generate all dates in the specified ranges\n",
    "    for start_date, end_date in date_ranges:\n",
    "        dates = get_dates_in_range(start_date, end_date)\n",
    "        all_dates.extend(dates)\n",
    "    \n",
    "    print(f\"Will download Congressional Record data for {len(all_dates)} dates\")\n",
    "    \n",
    "    # download data for each date\n",
    "    successful_downloads = 0\n",
    "    for date in tqdm(all_dates):\n",
    "        success = get_congressional_record(date)\n",
    "        if success:\n",
    "            successful_downloads += 1\n",
    "        \n",
    "        # wait between requests to avoid rate limiting\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"\\nData collection complete!\")\n",
    "    print(f\"Successfully downloaded data for {successful_downloads} out of {len(all_dates)} dates\")\n",
    "    print(f\"Data saved to: data/congressional_record/\")\n",
    "    \n",
    "    # create a simple summary file with immigration keywords to help with later analysis\n",
    "    with open('data/immigration_keywords.txt', 'w') as f:\n",
    "        keywords = [\n",
    "            'immigration', 'immigrant', 'migrant', 'migration', 'asylum', \n",
    "            'refugee', 'border', 'wall', 'undocumented', 'illegal alien',\n",
    "            'daca', 'dreamer', 'deportation', 'visa', 'citizenship',\n",
    "            'family separation', 'child detention', 'border security',\n",
    "            'border crisis', 'caravan', 'amnesty', 'path to citizenship'\n",
    "        ]\n",
    "        f.write('\\n'.join(keywords))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 206 files with immigration content\n",
      "First 5 files with immigration content:\n",
      "  - CREC-2019-01-02-CREC-2019-01-02-pt1-PgD1334-2.html: wall, mexico\n",
      "  - CREC-2019-01-02-CREC-2019-01-02-pt1-PgD1335-3.html: visa\n",
      "  - CREC-2019-01-02-CREC-2019-01-02-pt1-PgH10607-6.html: mexico\n",
      "  - CREC-2019-01-02-CREC-2019-01-02-pt1-PgH10607-8.html: mexico\n",
      "  - CREC-2019-01-02-CREC-2019-01-02-pt1-PgH10608.html: wall\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "path = r\"C:\\Users\\kevin barcenas\\Documents\\GitHub\\LIN350Project\\congressional_record\"\n",
    "html_files = glob.glob(os.path.join(path, \"*.html\"))\n",
    "\n",
    "# Expanded immigration terms list\n",
    "immigration_terms = [\n",
    "    'immigration', 'immigrant', 'border', 'asylum', 'refugee', \n",
    "    'undocumented', 'illegal alien', 'migrant', 'caravan',\n",
    "    'wall', 'daca', 'dreamer', 'deportation',\n",
    "    'family separation', 'mexico', 'visa', 'citizenship'\n",
    "]\n",
    "\n",
    "# Search ALL files (this might take a few minutes)\n",
    "immigration_files = []\n",
    "for file in html_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        try:\n",
    "            content = f.read().lower()\n",
    "            found_terms = [term for term in immigration_terms if term in content]\n",
    "            if found_terms:\n",
    "                immigration_files.append({\n",
    "                    'file': file,\n",
    "                    'terms': found_terms\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "print(f\"Found {len(immigration_files)} files with immigration content\")\n",
    "\n",
    "# If we found any, save the list for reference\n",
    "if immigration_files:\n",
    "    df = pd.DataFrame(immigration_files)\n",
    "    df.to_csv(os.path.join(path, \"immigration_files.csv\"), index=False)\n",
    "    print(\"First 5 files with immigration content:\")\n",
    "    for item in immigration_files[:5]:\n",
    "        print(f\"  - {os.path.basename(item['file'])}: {', '.join(item['terms'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
