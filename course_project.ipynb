{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIN350 Course Project - The Language of Immigration Politics: Terminology Differences Across Party Lines in Congressional Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way I usually run jupyter notebooks is opening the anaconda prompt terminal and running the command *jupyter notebook* from there I go to visual studio and click on select kernel -> existing jupyter server -> localhost or you can copy and paste the url of the tab that opened up with the *jupyter notebook* command and then click on python and that should be it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep track of the work we're doing together we can use a github repository to update changes and sync up our work. The usual workflow for this should be.\n",
    "1. Any changes you have in your laptop can be added to the repository with \"git add ./\" from the terminal the notebook is in\n",
    "2. After adding the files and changes you can use \"git commit -m 'message here'\" For the message make sure its in quotations and it can be anything\n",
    "3. After adding and commiting you can \"git push\" which pushes ur changes to the repository\n",
    "4. Let's say there's changes in the repository that are not in your laptop you can fetch them with \"git pull\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other setup you might need to do is set environement variables in local computer since we don't want to share that in the repository for privacy issues. So to do this you would run commands in your notebook to set it up. I'll show you\n",
    "1. running \"%env\" in a code block will show you all the environment variables in the jupyter environment\n",
    "2. to set up the enviroment variable for our project run the command \"%env API_KEY=apikeyfromourgoogledocs\"\n",
    "3. After that running the first cell of code will setup the api key to be used as API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Record Data Collector - Very simple for now, simple text data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: API_KEY=qAyZUrTJs4fdKIPefgekdiMQrCchdt979fIo58M1\n"
     ]
    }
   ],
   "source": [
    "%env API_KEY=qAyZUrTJs4fdKIPefgekdiMQrCchdt979fIo58M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SECTION 1: INTRODUCTION AND SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing project...\n",
      "\n",
      "Directory structure created:\n",
      "  - base_dir: c:\\Users\\Kevin\\Downloads\\LIN350Project\n",
      "  - data_dir: c:\\Users\\Kevin\\Downloads\\LIN350Project\\data\n",
      "  - raw_data_dir: c:\\Users\\Kevin\\Downloads\\LIN350Project\\data\\congressional_record\n",
      "  - processed_dir: c:\\Users\\Kevin\\Downloads\\LIN350Project\\processed_data\n",
      "  - samples_dir: c:\\Users\\Kevin\\Downloads\\LIN350Project\\processed_data\\speech_samples\n",
      "  - figures_dir: c:\\Users\\Kevin\\Downloads\\LIN350Project\\processed_data\\figures\n",
      "API key configured\n",
      "Constants defined:\n",
      "  - Date ranges: 8 periods\n",
      "  - Term pairs: 11 pairs/groups\n",
      "  - Immigration terms: 26 terms\n",
      "\n",
      "Project initialization complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Research Questions:\n",
    "\n",
    "1. What statistically significant differences exist in the frequency of \n",
    "immigration-related terminology (e.g., “undocumented” vs. “illegal”) between \n",
    "Democratic and Republican representatives?**\n",
    "\n",
    "2. How has party-specific immigration terminology shifted during key political \n",
    "events between 2017 and 2021, such as the DACA debates, the 2018–2019 government\n",
    "shutdown, or early Biden-era policy reversals?\n",
    "\n",
    "3. What contextual linguistic patterns (e.g., collocates, frames, sentiment) \n",
    "surround immigration terms in each party's discourse, and how do these reflect \n",
    "broader ideological narratives or legislative priorities?\n",
    "\n",
    "The project analyzes Congressional Record speeches to investigate how politicians from\n",
    "different parties use immigration-related terminology, following the methodologies\n",
    "covered in the LIN350 course.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import xlsxwriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "nltk.download('punkt') \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "\n",
    "def setup_directories():\n",
    "    # create all necessary directories for the project. returns a dictionary of important paths.\n",
    "    base_dir = os.path.join(os.getcwd())\n",
    "    \n",
    "    # main data directories\n",
    "    data_dir = os.path.join(base_dir, \"data\")\n",
    "    raw_data_dir = os.path.join(data_dir, \"congressional_record\")\n",
    "    processed_dir = os.path.join(base_dir, \"processed_data\")\n",
    "    samples_dir = os.path.join(processed_dir, \"speech_samples\")\n",
    "    figures_dir = os.path.join(processed_dir, \"figures\")  \n",
    "    \n",
    "    # create all directories\n",
    "    for directory in [data_dir, raw_data_dir, processed_dir, samples_dir, figures_dir]:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # return dictionary of paths for easy reference\n",
    "    return {\n",
    "        \"base_dir\": base_dir,\n",
    "        \"data_dir\": data_dir,\n",
    "        \"raw_data_dir\": raw_data_dir,\n",
    "        \"processed_dir\": processed_dir,\n",
    "        \"samples_dir\": samples_dir,\n",
    "        \"figures_dir\": figures_dir\n",
    "    }\n",
    "\n",
    "def setup_api_key():\n",
    "    # set up the API key for accessing the Congress.gov API. returns the API key.\n",
    "\n",
    "    # uncomment and run this line to set the API key in the notebook environment\n",
    "    # %env API_KEY=your_api_key_here\n",
    "    \n",
    "    try:\n",
    "        API_KEY = os.environ.get(\"API_KEY\")\n",
    "        if not API_KEY:\n",
    "            print(\"Warning: API_KEY environment variable not found.\")\n",
    "            print(\"Please run '%env API_KEY=your_api_key' in a cell.\")\n",
    "            return None\n",
    "        return API_KEY\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing API key: {e}\")\n",
    "        return None\n",
    "\n",
    "# define constants for data collection\n",
    "def define_constants():   \n",
    "    date_ranges = [\n",
    "        # 2019 - Border wall government shutdown\n",
    "        (\"2019-01-01\", \"2019-01-31\"),\n",
    "        \n",
    "        # Government shutdown over border wall funding\n",
    "        (\"2018-12-15\", \"2018-12-31\"),\n",
    "\n",
    "        # DACA debates\n",
    "        (\"2017-09-01\", \"2017-10-15\"),\n",
    "        (\"2018-01-15\", \"2018-02-15\"),\n",
    "        \n",
    "        # Border surge discussions\n",
    "        (\"2019-03-01\", \"2019-04-15\"),\n",
    "        \n",
    "        # Election year immigration discussions\n",
    "        (\"2020-01-15\", \"2020-02-15\"),\n",
    "        (\"2020-09-01\", \"2020-10-15\"),\n",
    "        \n",
    "        # Biden administration policy changes\n",
    "        (\"2021-01-20\", \"2021-03-01\")\n",
    "    ]\n",
    "    \n",
    "    # immigration-related term pairs for analysis\n",
    "    term_pairs = [\n",
    "        (\"undocumented\", \"illegal\", \"unauthorized\"),  # Status descriptors\n",
    "        (\"asylum seeker\", \"refugee\", \"migrant\"),      # Migration categories\n",
    "        (\"border security\", \"border crisis\", \"border management\"),  # Border framing\n",
    "        (\"path to citizenship\", \"amnesty\"),           # Legal status solutions\n",
    "        (\"dreamers\", \"daca recipients\"),              # Youth beneficiaries \n",
    "        (\"family separation\", \"child detention\"),      # Child policy framing\n",
    "        (\"chain migration\", \"family reunification\"),  # Family immigration framing\n",
    "        (\"alien\", \"foreign national\", \"noncitizen\", \"undocumented\"),  # Legal designation terms\n",
    "        (\"deportation\", \"removal\"),                   # Enforcement terminology\n",
    "        (\"sanctuary cities\", \"non-cooperative jurisdictions\"),  # Local policy framing\n",
    "        (\"border wall\", \"border barrier\", \"border infrastructure\")  # Border infrastructure\n",
    "    ]\n",
    "    \n",
    "    # immigration-related terms with more precise matching\n",
    "    immigration_terms = {\n",
    "        # regular terms - can appear within other words\n",
    "        'immigration': r'immigration',\n",
    "        'immigrant': r'immigrant',\n",
    "        'migrant': r'migrant',\n",
    "        'citizenship': r'citizenship',\n",
    "        'deportation': r'deportation',\n",
    "        \n",
    "        # terms that need word boundary checks\n",
    "        'border': r'\\b(?:border|borders)\\b',\n",
    "        'asylum': r'\\basylum\\b',\n",
    "        'refugee': r'\\b(?:refugee|refugees)\\b',\n",
    "        'undocumented': r'\\bundocumented\\b',\n",
    "        'illegal alien': r'\\billegal\\s+alien',\n",
    "        'unauthorized': r'\\bunauthorized\\b',\n",
    "        'wall': r'\\bwall\\b',\n",
    "        'daca': r'\\bdaca\\b',\n",
    "        'dreamer': r'\\b(?:dreamer|dreamers)\\b',\n",
    "        'visa': r'\\bvisa\\b',\n",
    "        'detention': r'\\bdetention\\b',\n",
    "        \n",
    "        # phrases\n",
    "        'family separation': r'family\\s+separation',\n",
    "        'child detention': r'child\\s+detention',\n",
    "        'border security': r'border\\s+security',\n",
    "        'border crisis': r'border\\s+crisis',\n",
    "        'path to citizenship': r'path\\s+to\\s+citizenship',\n",
    "        'amnesty': r'\\bamnesty\\b',\n",
    "        'caravan': r'\\bcaravan\\b',\n",
    "        \n",
    "        # specific entities\n",
    "        'mexico': r'\\bmexico\\b',\n",
    "        'ice': r'\\b(?:ice|immigration and customs enforcement)\\b',  # Only match whole word \"ice\"\n",
    "        'cbp': r'\\b(?:cbp|customs and border protection)\\b'\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"date_ranges\": date_ranges,\n",
    "        \"term_pairs\": term_pairs,\n",
    "        \"immigration_terms\": immigration_terms,\n",
    "    }\n",
    "\n",
    "# initialize the project\n",
    "def initialize_project():\n",
    "   \n",
    "    print(\"Initializing project...\\n\")\n",
    "    \n",
    "    directories = setup_directories()\n",
    "    print(f\"Directory structure created:\")\n",
    "    for name, path in directories.items():\n",
    "        print(f\"  - {name}: {path}\")\n",
    "    \n",
    "    api_key = setup_api_key()\n",
    "    if api_key:\n",
    "        print(f\"API key configured\")\n",
    "    \n",
    "    constants = define_constants()\n",
    "    print(f\"Constants defined:\")\n",
    "    print(f\"  - Date ranges: {len(constants['date_ranges'])} periods\")\n",
    "    print(f\"  - Term pairs: {len(constants['term_pairs'])} pairs/groups\")\n",
    "    print(f\"  - Immigration terms: {len(constants['immigration_terms'])} terms\")\n",
    "    \n",
    "    config = {\n",
    "        \"directories\": directories,\n",
    "        \"api_key\": api_key,\n",
    "        \"constants\": constants\n",
    "    }\n",
    "    \n",
    "    print(\"\\nProject initialization complete!\")\n",
    "    return config\n",
    "\n",
    "# run initialization\n",
    "config = initialize_project()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SECTION 2: DATA COLLECTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API key with GovInfo API...\n",
      "Success! Your API key is valid for the GovInfo API.\n",
      "Status code: 200\n",
      "\n",
      "Available collections:\n",
      "- Congressional Bills\n",
      "- Congressional Bill Status\n",
      "- Congressional Bill Summaries\n",
      "- United States Budget\n",
      "- Congressional Calendars\n",
      "Will download Congressional Record data for 289 dates\n",
      "\n",
      "Data collection complete!\n",
      "Successfully downloaded data for 0 out of 289 dates\n",
      "Data saved to: c:\\Users\\Kevin\\Downloads\\LIN350Project\\data\\congressional_record\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# function to generate all dates in a given range\n",
    "def get_dates_in_range(start_date, end_date):\n",
    "    # start_date (str): Start date in format 'YYYY-MM-DD'\n",
    "    # end_date (str): End date in format 'YYYY-MM-DD'\n",
    "        \n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    date_list = []\n",
    "    current = start\n",
    "    while current <= end:\n",
    "        date_list.append(current.strftime(\"%Y-%m-%d\"))\n",
    "        current += timedelta(days=1)\n",
    "    return date_list\n",
    "\n",
    "def verify_api_key(api_key):\n",
    "\n",
    "    test_url = \"https://api.govinfo.gov/collections\"\n",
    "    params = {\n",
    "        'api_key': api_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"Testing API key with GovInfo API...\")\n",
    "        response = requests.get(test_url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"Success! Your API key is valid for the GovInfo API.\")\n",
    "            print(f\"Status code: {response.status_code}\")\n",
    "            \n",
    "            # show the first few collections to confirm we got real data\n",
    "            collections = response.json().get('collections', [])\n",
    "            if collections:\n",
    "                print(\"\\nAvailable collections:\")\n",
    "                for collection in collections[:5]:\n",
    "                    print(f\"- {collection.get('collectionName', 'Unknown')}\")\n",
    "            return True\n",
    "            \n",
    "        elif response.status_code == 401 or response.status_code == 403:\n",
    "            print(\"Authentication failed. Your API key appears to be invalid.\")\n",
    "            print(f\"Status code: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"Received unexpected status code: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while testing the API key: {e}\")\n",
    "        return False\n",
    "\n",
    "# function to get Congressional Record data using the GovInfo API\n",
    "def get_congressional_record(date, api_key, raw_data_dir):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        date (str): Date in format 'YYYY-MM-DD'\n",
    "        api_key (str): API key for the GovInfo API\n",
    "        raw_data_dir (str): Directory to save raw data  \n",
    "    \"\"\"\n",
    "    package_id = f\"CREC-{date}\"\n",
    "    package_url = f\"https://api.govinfo.gov/packages/{package_id}/summary\"\n",
    "    params = {\n",
    "        'api_key': api_key\n",
    "    }\n",
    "    try:\n",
    "        # check if the package exists\n",
    "        response = requests.get(package_url, params=params)\n",
    "    \n",
    "        # if package doesn't exist or other error\n",
    "        if response.status_code != 200:\n",
    "            print(f\"No Congressional Record available for {date} (Status: {response.status_code})\")\n",
    "            return False\n",
    "        \n",
    "        # save the package summary\n",
    "        with open(os.path.join(raw_data_dir, f\"{package_id}-summary.json\"), 'w') as f:\n",
    "            json.dump(response.json(), f)\n",
    "        \n",
    "        # get granules (speeches and entries) \n",
    "        granules_url = f\"https://api.govinfo.gov/packages/{package_id}/granules\"\n",
    "        granules_params = {\n",
    "            'api_key': api_key,\n",
    "            'offset': 0,\n",
    "            'pageSize': 100  # Max page size\n",
    "        }\n",
    "        \n",
    "        # get first page of granules\n",
    "        granules_response = requests.get(granules_url, params=granules_params)\n",
    "        \n",
    "        if granules_response.status_code != 200:\n",
    "            print(f\"Failed to get granules for {date} (Status: {granules_response.status_code})\")\n",
    "            return False\n",
    "            \n",
    "        # save the granules list\n",
    "        with open(os.path.join(raw_data_dir, f\"{package_id}-granules.json\"), 'w') as f:\n",
    "            json.dump(granules_response.json(), f)\n",
    "            \n",
    "        # download content for each granule\n",
    "        granules = granules_response.json().get('granules', [])\n",
    "        \n",
    "        for granule in granules:\n",
    "            granule_id = granule.get('granuleId')\n",
    "            \n",
    "            # skip if no granule ID\n",
    "            if not granule_id:\n",
    "                continue\n",
    "            \n",
    "            # get the HTML content\n",
    "            content_url = f\"https://api.govinfo.gov/packages/{package_id}/granules/{granule_id}/htm\"\n",
    "            content_response = requests.get(content_url, params=params)\n",
    "            \n",
    "            if content_response.status_code == 200:\n",
    "                # save the HTML content\n",
    "                with open(os.path.join(raw_data_dir, f\"{package_id}-{granule_id}.html\"), 'w', encoding='utf-8') as f:\n",
    "                    f.write(content_response.text)\n",
    "            \n",
    "            # respect rate limit\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        print(f\"Successfully downloaded Congressional Record for {date} ({len(granules)} granules)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data for {date}: {e}\")\n",
    "        return False\n",
    "\n",
    "# main function to download Congressional Record data\n",
    "def collect_congressional_data(config):\n",
    "    \"\"\"\n",
    "    Args: config (dict): Project configuration\n",
    "    Returns: int: Number of successfully downloaded dates\n",
    "    \"\"\"\n",
    "    api_key = config[\"api_key\"]\n",
    "    date_ranges = config[\"constants\"][\"date_ranges\"]\n",
    "    raw_data_dir = config[\"directories\"][\"raw_data_dir\"]\n",
    "    \n",
    "    if not verify_api_key(api_key):\n",
    "        print(\"Cannot proceed with data collection due to invalid API key.\")\n",
    "        return 0\n",
    "    \n",
    "    all_dates = []\n",
    "    \n",
    "    # generate all dates in the specified ranges\n",
    "    for start_date, end_date in date_ranges:\n",
    "        dates = get_dates_in_range(start_date, end_date)\n",
    "        all_dates.extend(dates)\n",
    "    \n",
    "    print(f\"Will download Congressional Record data for {len(all_dates)} dates\")\n",
    "    \n",
    "    # download data for each date, commented out because I we already collected the data\n",
    "    successful_downloads = 0\n",
    "    # for date in tqdm(all_dates, desc=\"Downloading Congressional Records\"):\n",
    "    #     success = get_congressional_record(date, api_key, raw_data_dir)\n",
    "    #     if success:\n",
    "    #         successful_downloads += 1\n",
    "        \n",
    "    #     # Wait between requests to avoid rate limiting\n",
    "    #     time.sleep(1)\n",
    "    \n",
    "    print(f\"\\nData collection complete!\")\n",
    "    print(f\"Successfully downloaded data for {successful_downloads} out of {len(all_dates)} dates\")\n",
    "    print(f\"Data saved to: {raw_data_dir}\")\n",
    "    \n",
    "    return successful_downloads\n",
    "\n",
    "# Uncomment to run data collection\n",
    "successful_downloads = collect_congressional_data(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 2010 records from 796 unique legislators\n"
     ]
    }
   ],
   "source": [
    "# build a df of legislators from the @unitedstates Github data (2017-2022)\n",
    "current_file = \"data\\\\legislator_data\\\\unitedstates.github.io\\\\legislators-current.json\"\n",
    "historical_file = \"data\\\\legislator_data\\\\unitedstates.github.io\\\\legislators-historical.json\"\n",
    "def build_legislators_dataframe(current_file=current_file, \n",
    "                               historical_file=historical_file):\n",
    "   \n",
    "    with open(current_file, 'r') as f:\n",
    "        current = json.load(f)\n",
    "    \n",
    "    with open(historical_file, 'r') as f:\n",
    "        historical = json.load(f)\n",
    "    \n",
    "    all_legislators = current + historical\n",
    "    legislator_records = []\n",
    "    \n",
    "    study_start = datetime.strptime('2017-01-01', '%Y-%m-%d')\n",
    "    study_end = datetime.strptime('2022-12-31', '%Y-%m-%d')\n",
    "    \n",
    "    for legislator in all_legislators:\n",
    "        # legislator info\n",
    "        legislator_id = legislator.get('id', {}).get('bioguide', '')\n",
    "        first_name = legislator.get('name', {}).get('first', '')\n",
    "        last_name = legislator.get('name', {}).get('last', '')\n",
    "        \n",
    "        # process each term to see if any fall within our study period\n",
    "        for term in legislator.get('terms', []):\n",
    "            term_start = datetime.strptime(term.get('start', '1900-01-01'), '%Y-%m-%d')\n",
    "            term_end = datetime.strptime(term.get('end', '2100-01-01'), '%Y-%m-%d')\n",
    "            \n",
    "            # check if this term overlaps with our study period\n",
    "            if (term_start <= study_end and term_end >= study_start):\n",
    "                record = {\n",
    "                    'bioguide_id': legislator_id,\n",
    "                    'first_name': first_name,\n",
    "                    'last_name': last_name,\n",
    "                    'last_name_upper': last_name.upper(),  # For easier matching\n",
    "                    'full_name': legislator.get('name', {}).get('official_full', f\"{first_name} {last_name}\"),\n",
    "                    'state': term.get('state', ''),\n",
    "                    'party': term.get('party', ''),\n",
    "                    'type': term.get('type', ''),  # 'sen' or 'rep'\n",
    "                    'term_start': term.get('start', ''),\n",
    "                    'term_end': term.get('end', ''),\n",
    "                    'state_rank': term.get('state_rank', '')  # 'junior' or 'senior' for senators\n",
    "                }\n",
    "                legislator_records.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(legislator_records)\n",
    "    df.to_csv('legislators_2017_2022.csv', index=False)\n",
    "    print(f\"Created DataFrame with {len(df)} records from {len(set(df['bioguide_id']))} unique legislators\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "legislators_df = build_legislators_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract speaker information and determine party from Congressional Record text\n",
    "# returns party and details about how the match was made\n",
    "def get_party_from_speech(speech_text, legislators_df):\n",
    "    # regex\n",
    "    speaker_match = re.search(r'(?:Mr\\.|Mrs\\.|Ms\\.|Representative|Senator)\\s+([A-Z][A-Za-z\\'\\-]+)(?:\\s+of\\s+([A-Za-z\\s]+))?', speech_text)\n",
    "    \n",
    "    if not speaker_match:\n",
    "        return None, \"No speaker pattern found\"\n",
    "    \n",
    "    last_name = speaker_match.group(1)\n",
    "    state_name = speaker_match.group(2)\n",
    "    \n",
    "    matches = legislators_df[legislators_df['last_name_upper'] == last_name]\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        with open(\"unmatched_speakers.txt\", \"a\") as log_file:\n",
    "            log_file.write(f\"{last_name} | From: {speech_text[:200]}...\\n\")\n",
    "        return None, f\"No match found for {last_name}\"\n",
    "\n",
    "    \n",
    "    if len(matches) == 1:\n",
    "        # single match - straightforward case\n",
    "        return matches.iloc[0]['party'], \"Unique last name match\"\n",
    "    \n",
    "    # multiple matches - try to narrow down with state\n",
    "    if state_name:\n",
    "        # convert full state name to abbreviation\n",
    "        state_abbrev = state_name_to_abbrev(state_name)\n",
    "        state_matches = matches[matches['state'] == state_abbrev]\n",
    "        \n",
    "        if len(state_matches) == 1:\n",
    "            return state_matches.iloc[0]['party'], f\"Resolved with state ({state_abbrev})\"\n",
    "        elif len(state_matches) > 1:\n",
    "            # still multiple matches with same state\n",
    "            # sort by term_end to get the most recent/current legislator\n",
    "            recent_match = state_matches.sort_values('term_end', ascending=False).iloc[0]\n",
    "            return recent_match['party'], f\"Multiple matches with state, using most recent ({recent_match['full_name']})\"\n",
    "    \n",
    "    # no state or state didn't narrow it down - use most recent term\n",
    "    recent_match = matches.sort_values('term_end', ascending=False).iloc[0]\n",
    "    return recent_match['party'], f\"Multiple matches, using most recent ({recent_match['full_name']})\"\n",
    "\n",
    "# helper function to get state name to abbrev\n",
    "def state_name_to_abbrev(state_name):\n",
    "    state_name = state_name.strip().lower()\n",
    "    states = {\n",
    "        'alabama': 'AL', 'alaska': 'AK', 'arizona': 'AZ', 'arkansas': 'AR', 'california': 'CA',\n",
    "        'colorado': 'CO', 'connecticut': 'CT', 'delaware': 'DE', 'florida': 'FL', 'georgia': 'GA',\n",
    "        'hawaii': 'HI', 'idaho': 'ID', 'illinois': 'IL', 'indiana': 'IN', 'iowa': 'IA',\n",
    "        'kansas': 'KS', 'kentucky': 'KY', 'louisiana': 'LA', 'maine': 'ME', 'maryland': 'MD',\n",
    "        'massachusetts': 'MA', 'michigan': 'MI', 'minnesota': 'MN', 'mississippi': 'MS', 'missouri': 'MO',\n",
    "        'montana': 'MT', 'nebraska': 'NE', 'nevada': 'NV', 'new hampshire': 'NH', 'new jersey': 'NJ',\n",
    "        'new mexico': 'NM', 'new york': 'NY', 'north carolina': 'NC', 'north dakota': 'ND', 'ohio': 'OH',\n",
    "        'oklahoma': 'OK', 'oregon': 'OR', 'pennsylvania': 'PA', 'rhode island': 'RI', 'south carolina': 'SC',\n",
    "        'south dakota': 'SD', 'tennessee': 'TN', 'texas': 'TX', 'utah': 'UT', 'vermont': 'VT',\n",
    "        'virginia': 'VA', 'washington': 'WA', 'west virginia': 'WV', 'wisconsin': 'WI', 'wyoming': 'WY'\n",
    "    }\n",
    "    \n",
    "    return states.get(state_name, state_name)\n",
    "\n",
    "# example usage:\n",
    "def analyze_speech_by_party(speech_text, legislators_df, term_pairs):\n",
    "    \"\"\"\n",
    "    Analyze usage of immigration term pairs in a speech text\n",
    "    and attribute them to the speaker's party\n",
    "    \"\"\"\n",
    "    party, match_details = get_party_from_speech(speech_text, legislators_df)\n",
    "    \n",
    "    if not party:\n",
    "        return None, f\"Could not determine party: {match_details}\"\n",
    "    \n",
    "    # init. dictionary to store term counts by party\n",
    "    term_counts = {term: 0 for pair in term_pairs for term in pair}\n",
    "    \n",
    "    # count occurrences of each term in the speech\n",
    "    for pair in term_pairs:\n",
    "        for term in pair:\n",
    "            # count how many times the term appears in the speech (case insensitive)\n",
    "            count = len(re.findall(r'\\b' + re.escape(term) + r'\\b', speech_text, re.IGNORECASE))\n",
    "            term_counts[term] = count\n",
    "    \n",
    "    return party, term_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SECTION 3: DATA PROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to identify immigration-related files\n",
    "def identify_immigration_files(config):\n",
    "    \"\"\"\n",
    "    searches through all HTML files and identifies those containing immigration-related terms.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Project configuration\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame of immigration-related files\n",
    "    \"\"\"\n",
    "    raw_data_dir = config[\"directories\"][\"raw_data_dir\"]\n",
    "    processed_dir = config[\"directories\"][\"processed_dir\"]\n",
    "    immigration_terms = config[\"constants\"][\"immigration_terms\"]\n",
    "    \n",
    "    # list of all HTML files\n",
    "    html_files = glob.glob(os.path.join(raw_data_dir, \"*.html\"))\n",
    "    total_files = len(html_files)\n",
    "    \n",
    "    print(f\"Found {total_files} HTML files in {raw_data_dir}\")\n",
    "    \n",
    "    # check the first few files to make sure we can access them\n",
    "    if total_files > 0:\n",
    "        print(\"\\nSample filenames:\")\n",
    "        for file in html_files[:5]:\n",
    "            print(f\"  - {os.path.basename(file)}\")\n",
    "        \n",
    "        # try to open one file to verify access\n",
    "        try:\n",
    "            with open(html_files[0], 'r', encoding='utf-8') as f:\n",
    "                first_chars = f.read(200)\n",
    "            print(\"\\nSuccessfully read first file. First 200 characters:\")\n",
    "            print(first_chars.replace('\\n', ' ')[:200])\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError reading file: {e}\")\n",
    "    \n",
    "    # search for immigration-related content\n",
    "    immigration_files = []\n",
    "    print(f\"Searching {total_files} files for immigration content...\")\n",
    "\n",
    "    for file in tqdm(html_files, desc=\"Searching files for immigration terms\"):\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read().lower()\n",
    "                \n",
    "            # check each term with its specific regex pattern\n",
    "            found_terms = []\n",
    "            for term, pattern in immigration_terms.items():\n",
    "                if re.search(pattern, content):\n",
    "                    found_terms.append(term)\n",
    "            \n",
    "            if found_terms:\n",
    "                # extract date from filename\n",
    "                filename = os.path.basename(file)\n",
    "                date_parts = filename.split('-')\n",
    "                if len(date_parts) >= 2:\n",
    "                    date = date_parts[1]\n",
    "                else:\n",
    "                    date = \"Unknown\"\n",
    "                \n",
    "                immigration_files.append({\n",
    "                    'file': file,\n",
    "                    'date': date,\n",
    "                    'terms': ', '.join(found_terms)  # convert list to string\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {os.path.basename(file)}: {e}\")\n",
    "\n",
    "    # save results to CSV\n",
    "    if immigration_files:\n",
    "        immigration_df = pd.DataFrame(immigration_files)\n",
    "        csv_path = os.path.join(processed_dir, \"immigration_files.csv\")\n",
    "        immigration_df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        print(f\"\\nFound {len(immigration_files)} files with immigration content\")\n",
    "        print(f\"List saved to: {csv_path}\")\n",
    "        \n",
    "        # show sample of found files\n",
    "        print(\"\\nSample immigration-related files:\")\n",
    "        for file_info in immigration_files[:5]:\n",
    "            print(f\"  - {os.path.basename(file_info['file'])}: {file_info['terms']}\")\n",
    "    else:\n",
    "        print(\"No immigration-related files found.\")\n",
    "        immigration_df = pd.DataFrame()\n",
    "    \n",
    "    return immigration_df\n",
    "\n",
    "# function to extract structured data from HTML\n",
    "def parse_congressional_record(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        # Try to use <pre>, fallback to full text if missing\n",
    "        pre_content = soup.find('pre')\n",
    "        if pre_content:\n",
    "            text_block = pre_content.get_text()\n",
    "        else:\n",
    "            text_block = soup.get_text()\n",
    "            with open(\"parse_failures.txt\", \"a\") as log_file:\n",
    "                log_file.write(f\"{os.path.basename(file_path)} — Missing <pre>, using fallback\\n\")\n",
    "\n",
    "        full_text = text_block\n",
    "\n",
    "        # Title\n",
    "        title_tag = soup.find('title')\n",
    "        page_title = title_tag.get_text() if title_tag else \"Unknown\"\n",
    "\n",
    "        # Header-based metadata\n",
    "        header_text = pre_content.contents[0] if pre_content and pre_content.contents else \"\"\n",
    "        date_match = re.search(r'\\[Congressional Record Volume \\d+, Number \\d+ \\(([^)]+)\\)\\]', str(header_text))\n",
    "        date = date_match.group(1) if date_match else \"Unknown\"\n",
    "\n",
    "        chamber_lines = [line for line in str(header_text).split('\\n') if '[House]' in line or '[Senate]' in line]\n",
    "        chamber = \"House\" if chamber_lines and '[House]' in chamber_lines[0] else \\\n",
    "                  \"Senate\" if chamber_lines and '[Senate]' in chamber_lines[0] else \"Unknown\"\n",
    "\n",
    "        # Links (safe fallback)\n",
    "        links = []\n",
    "        link_tags = pre_content.find_all('a') if pre_content else soup.find_all('a')\n",
    "        for a_tag in link_tags:\n",
    "            href = a_tag.get('href', '')\n",
    "            text = a_tag.get_text()\n",
    "            links.append({\"href\": href, \"text\": text})\n",
    "\n",
    "        # Speaker info (main + fallback)\n",
    "        speaker_section = re.search(r'\\(((?:Mr\\.|Mrs\\.|Ms\\.|Senator|Representative)\\s+[A-Z][A-Za-z\\'\\-]+.*?)\\)', full_text)\n",
    "\n",
    "        speaker_full = \"Unknown\"\n",
    "        speaker_last = \"Unknown\"\n",
    "\n",
    "        if speaker_section:\n",
    "            speaker_text = speaker_section.group(1)\n",
    "            speaker_match = re.search(r'(Mr\\.|Mrs\\.|Ms\\.|Senator|Representative)\\s+([A-Z][A-Za-z\\'\\-]+)', speaker_text)\n",
    "            if speaker_match:\n",
    "                speaker_full = speaker_match.group(0)\n",
    "                speaker_last = speaker_match.group(2)\n",
    "\n",
    "        if speaker_full == \"Unknown\":\n",
    "            alt_match = re.search(r'(Mr\\.|Mrs\\.|Ms\\.|Senator|Representative)\\s+([A-Z][A-Za-z\\'\\-]+)', full_text)\n",
    "            if alt_match:\n",
    "                speaker_full = alt_match.group(0)\n",
    "                speaker_last = alt_match.group(2)\n",
    "\n",
    "        # Title\n",
    "        title = \"Unknown\"\n",
    "        title_section = re.search(r'to address the House[^.]*\\.\\)\\s+([A-Z][A-Z\\s\\'\",.()-]+?)\\s*\\n', full_text)\n",
    "        if title_section:\n",
    "            title = title_section.group(1).strip()\n",
    "\n",
    "        # Page number\n",
    "        filename = os.path.basename(file_path)\n",
    "        granule_id = filename.replace(\".html\", \"\")\n",
    "        page_match = re.search(r'\\[Page ([^\\]]+)\\]', full_text)\n",
    "        page_number = page_match.group(1) if page_match else \"Unknown\"\n",
    "\n",
    "        return {\n",
    "            'file_id': granule_id,\n",
    "            'date': date,\n",
    "            'chamber': chamber,\n",
    "            'speaker_full': speaker_full,\n",
    "            'speaker_last': speaker_last,\n",
    "            'title': title,\n",
    "            'page_number': page_number,\n",
    "            'links': links,\n",
    "            'page_title': page_title,\n",
    "            'full_text': full_text\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {os.path.basename(file_path)}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process all immigration-related files\n",
    "def process_immigration_files(config, immigration_df=None):\n",
    "    \"\"\"\n",
    "    Process all immigration-related files to extract structured data.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Project configuration\n",
    "        immigration_df (pandas.DataFrame, optional): df of immigration-related files\n",
    "            If None, the function will try to load it from a file\n",
    "            \n",
    "    Returns:\n",
    "        pandas.DataFrame: df of processed immigration speeches\n",
    "    \"\"\"\n",
    "    processed_dir = config[\"directories\"][\"processed_dir\"]\n",
    "    \n",
    "    # if no df is provided, try to load it from a file\n",
    "    if immigration_df is None:\n",
    "        immigration_files_csv = os.path.join(processed_dir, \"immigration_files.csv\")\n",
    "        if not os.path.exists(immigration_files_csv):\n",
    "            print(f\"Error: Immigration files list not found at {immigration_files_csv}\")\n",
    "            return None\n",
    "        \n",
    "        immigration_df = pd.read_csv(immigration_files_csv)\n",
    "    \n",
    "    print(f\"Processing {len(immigration_df)} immigration-related files...\")\n",
    "    \n",
    "    # process each file in the immigration list\n",
    "    parsed_data = []\n",
    "    for _, row in tqdm(immigration_df.iterrows(), total=len(immigration_df), desc=\"Parsing HTML files\"):\n",
    "        file_path = row['file']\n",
    "        extracted_data = parse_congressional_record(file_path)\n",
    "        \n",
    "        if extracted_data:\n",
    "            # add the immigration terms found\n",
    "            extracted_data['immigration_terms'] = row['terms']\n",
    "            parsed_data.append(extracted_data)\n",
    "    \n",
    "    # create a df and save to CSV\n",
    "    if parsed_data:\n",
    "        parsed_df = pd.DataFrame(parsed_data)\n",
    "        csv_path = os.path.join(processed_dir, \"immigration_speeches.csv\")\n",
    "        parsed_df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        print(f\"\\nSuccessfully parsed {len(parsed_data)} files\")\n",
    "        print(f\"Data saved to: {csv_path}\")\n",
    "        \n",
    "        # print summary of speakers found\n",
    "        speaker_counts = parsed_df['speaker_last'].value_counts()\n",
    "        print(f\"\\nTop 10 speakers in the dataset:\")\n",
    "        print(speaker_counts.head(10))\n",
    "        \n",
    "        # print example of first record\n",
    "        print(\"\\nExample of parsed data (first record):\")\n",
    "        for key, value in parsed_data[0].items():\n",
    "            if key == 'full_text':\n",
    "                print(f\"{key}: {value[:200]}...\") # Print only first 200 chars of text\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(\"No data could be parsed from the files.\")\n",
    "        parsed_df = pd.DataFrame()\n",
    "    \n",
    "    return parsed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean and improve data\n",
    "def clean_data(df, config):\n",
    "    \"\"\"\n",
    "    Clean and enhance the parsed data.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame of parsed speeches\n",
    "        config (dict): Project configuration\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame of all cleaned records, DataFrame of actual speeches only)\n",
    "    \"\"\"\n",
    "    # create a copy to avoid modifying the original\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # 1. Convert dates to standard format\n",
    "    def standardize_date(date_str):\n",
    "        try:\n",
    "            if pd.isna(date_str) or date_str.strip().lower() == \"unknown\":\n",
    "                raise ValueError(\"Missing or unknown date\")\n",
    "\n",
    "            date_obj = datetime.strptime(date_str.strip(), \"%A, %B %d, %Y\")\n",
    "            return date_obj.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            with open(\"date_parse_errors.txt\", \"a\") as log_file:\n",
    "                log_file.write(f\"❌ Could not parse date: '{date_str}' — {e}\\n\")\n",
    "            return None \n",
    "\n",
    "    \n",
    "    cleaned_df['date_standard'] = cleaned_df['date'].apply(standardize_date)\n",
    "    \n",
    "    # 2. Identify real speeches vs. procedural text\n",
    "    def is_real_speech(row):\n",
    "        if row['speaker_last'] == \"Unknown\":\n",
    "            return False\n",
    "\n",
    "        procedural_titles = [\n",
    "            'HOUSE', 'SENATE', 'PRAYER', 'PLEDGE', 'ADJOURNMENT', \n",
    "            'RECESS', 'AMENDMENT', 'RECORD', 'MOTION', 'RESOLUTION'\n",
    "        ]\n",
    "\n",
    "        title = row['title'].upper() if isinstance(row['title'], str) else \"\"\n",
    "        if any(keyword in title for keyword in procedural_titles):\n",
    "            return False\n",
    "\n",
    "        word_count = len(row['full_text'].split())\n",
    "        if word_count < 50:\n",
    "            # Only exclude short speeches if also marked as procedural\n",
    "            if \"adjourn\" in title.lower() or \"recess\" in title.lower():\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "    \n",
    "    cleaned_df['is_speech'] = cleaned_df.apply(is_real_speech, axis=1)\n",
    "    \n",
    "    # 3. Categorize speech type\n",
    "    def categorize_speech_ranked(row):\n",
    "        text = row.get('full_text', '')\n",
    "        if not isinstance(text, str):\n",
    "            return \"other\"\n",
    "        text = text.lower()\n",
    "        score = {}\n",
    "\n",
    "        categories = {\n",
    "            \"border_security\": [\"border security\", \"border wall\", \"border crisis\"],\n",
    "            \"legal_status\": [\"undocumented\", \"illegal alien\", \"unauthorized\", \"amnesty\", \"path to citizenship\"],\n",
    "            \"children\": [\"daca\", \"dreamer\", \"child\", \"family separation\"],\n",
    "            \"asylum\": [\"asylum\", \"refugee\", \"humanitarian\"],\n",
    "            \"general\": [\"immigration\", \"immigrant\", \"migrant\"]\n",
    "        }\n",
    "\n",
    "        for category, terms in categories.items():\n",
    "            count = sum(text.count(term) for term in terms)\n",
    "            if count > 0:\n",
    "                score[category] = count\n",
    "\n",
    "        return max(score, key=score.get) if score else \"other\"\n",
    "\n",
    "    \n",
    "    cleaned_df['speech_category'] = cleaned_df.apply(categorize_speech_ranked, axis=1)\n",
    "    \n",
    "    # 4. Extract a summary from the full text (first 300 characters)\n",
    "    def extract_summary(text):\n",
    "        # remove header content in square brackets\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        # remove whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # take first 300 characters\n",
    "        return text[:300] + \"...\" if len(text) > 300 else text\n",
    "    \n",
    "    cleaned_df['speech_summary'] = cleaned_df['full_text'].apply(extract_summary)\n",
    "    \n",
    "    # 5. add party information the legislator matching\n",
    "    def get_party_info(row):\n",
    "        \"\"\"Get party information for the speech using the speaker matching function\"\"\"\n",
    "        party, match_details = get_party_from_speech(row['full_text'], legislators_df)\n",
    "        return party if party else None\n",
    "    \n",
    "    if 'party' not in cleaned_df.columns or cleaned_df['party'].isna().any():\n",
    "        print(\"ℹ️ Filling in missing party values using speech text...\")\n",
    "        \n",
    "        def get_party_if_missing(row):\n",
    "            current_party = row.get('party', None)\n",
    "            if pd.isna(current_party):\n",
    "                party, _ = get_party_from_speech(row['full_text'], legislators_df)\n",
    "                return party\n",
    "            return current_party\n",
    "\n",
    "        \n",
    "        cleaned_df['party'] = cleaned_df.apply(get_party_if_missing, axis=1)\n",
    "    else:\n",
    "        print(\"✅ Using existing party data (no recomputation needed)\")\n",
    "            \n",
    "    # 6. Count tokens (words) in each speech\n",
    "    def count_tokens(text):\n",
    "        try:\n",
    "            # nltk's\n",
    "            return len(word_tokenize(text))\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    cleaned_df['token_count'] = cleaned_df['full_text'].apply(count_tokens)\n",
    "    \n",
    "    # 7. Count sentences in each speech\n",
    "    def count_sentences(text):\n",
    "        try:\n",
    "            # nltk's \n",
    "            return len(sent_tokenize(text))\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    cleaned_df['sentence_count'] = cleaned_df['full_text'].apply(count_sentences)\n",
    "    \n",
    "    # 8. Add page number from the improved parser\n",
    "    if 'page_number' in df.columns:\n",
    "        cleaned_df['page_number'] = df['page_number']\n",
    "    \n",
    "    # 9. Extract links from the improved parser if available\n",
    "    if 'links' in df.columns:\n",
    "        cleaned_df['links'] = df['links']\n",
    "    \n",
    "    # 10. Keep only relevant columns in a useful order\n",
    "    columns_order = [\n",
    "        'file_id', 'date_standard', 'chamber', 'speaker_full', 'speaker_last', \n",
    "        'party', 'title', 'is_speech', 'speech_category', 'speech_summary', \n",
    "        'token_count', 'sentence_count', 'page_number', 'immigration_terms', 'full_text'\n",
    "    ]\n",
    "    \n",
    "    # filter columns that actually exist in the DataFrame\n",
    "    columns_order = [col for col in columns_order if col in cleaned_df.columns]\n",
    "    \n",
    "    # return only the columns we want and create filtered dataset with only actual speeches\n",
    "    cleaned_df = cleaned_df[columns_order]\n",
    "    speeches_only = cleaned_df[cleaned_df['is_speech'] == True]\n",
    "    \n",
    "    return (cleaned_df, speeches_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following runs the data processing pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14629 HTML files in c:\\Users\\Kevin\\Downloads\\LIN350Project\\data\\congressional_record\n",
      "\n",
      "Sample filenames:\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-2.html\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-3.html\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-4.html\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-5.html\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-6.html\n",
      "\n",
      "Successfully read first file. First 200 characters:\n",
      "<html> <head> <title>Congressional Record, Volume 163 Issue 141 (Friday, September 1, 2017)</title> </head> <body><pre> [Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)] [Daily\n",
      "Searching 14629 files for immigration content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de7c286ea74a46e4bc59c343b4848f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Searching files for immigration terms:   0%|          | 0/14629 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1785 files with immigration content\n",
      "List saved to: c:\\Users\\Kevin\\Downloads\\LIN350Project\\processed_data\\immigration_files.csv\n",
      "\n",
      "Sample immigration-related files:\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-6.html: visa\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgE1151-4.html: refugee\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgE1152-3.html: immigration, immigrant, migrant, citizenship, deportation, undocumented, daca, dreamer, visa\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgE1154-4.html: undocumented, mexico\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgH6632-6.html: mexico\n",
      "Processing 1785 immigration-related files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af40dfe02a54d63847d58f951fce331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing HTML files:   0%|          | 0/1785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully parsed 1785 files\n",
      "Data saved to: c:\\Users\\Kevin\\Downloads\\LIN350Project\\processed_data\\immigration_speeches.csv\n",
      "\n",
      "Top 10 speakers in the dataset:\n",
      "speaker_last\n",
      "Unknown      192\n",
      "McCONNELL     94\n",
      "SCHUMER       39\n",
      "CORNYN        30\n",
      "DURBIN        28\n",
      "FEINSTEIN     26\n",
      "CARDIN        18\n",
      "Ernst         17\n",
      "GRASSLEY      16\n",
      "MERKLEY       14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Example of parsed data (first record):\n",
      "file_id: CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-6\n",
      "date: Friday, September 1, 2017\n",
      "chamber: Unknown\n",
      "speaker_full: Unknown\n",
      "speaker_last: Unknown\n",
      "title: Unknown\n",
      "page_number: D910\n",
      "links: [{'href': 'https://www.gpo.gov', 'text': 'www.gpo.gov'}, {'href': 'http://www.govinfo.gov', 'text': 'www.govinfo.gov'}, {'href': 'mailto:contactcenter@gpo.gov', 'text': 'contactcenter@gpo.gov'}]\n",
      "page_title: Congressional Record, Volume 163 Issue 141 (Friday, September 1, 2017)\n",
      "full_text: \n",
      "[Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)]\n",
      "[Daily Digest]\n",
      "[Pages D909-D910]\n",
      "From the Congressional Record Online through the Government Publishing Office [www.gpo.gov]\n",
      "...\n",
      "immigration_terms: visa\n",
      "ℹ️ Filling in missing party values using speech text...\n",
      "\n",
      "Created cleaned dataset with 1785 records\n",
      "Created filtered dataset with 1593 actual speeches\n",
      "\n",
      "Party distribution in speeches:\n",
      "party\n",
      "Democrat       577\n",
      "Republican     496\n",
      "Independent     16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Speech category distribution:\n",
      "speech_category\n",
      "children           617\n",
      "other              421\n",
      "general            308\n",
      "asylum             100\n",
      "border_security     96\n",
      "legal_status        51\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# run the data processing pipeline\n",
    "def run_data_processing(config):\n",
    "    \"\"\"\n",
    "    Run the data processing pipeline, focusing only on preprocessing.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Project configuration\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame of all cleaned records, DataFrame of speeches only)\n",
    "    \"\"\"\n",
    "    # Step 1: Identify immigration-related files\n",
    "    immigration_df = identify_immigration_files(config)\n",
    "    \n",
    "    # Step 2: Process immigration-related files\n",
    "    speeches_df = process_immigration_files(config, immigration_df)\n",
    "    \n",
    "    # Step 3: Clean and enhance the data\n",
    "    if speeches_df is not None and not speeches_df.empty:\n",
    "        cleaned_df, speeches_only = clean_data(speeches_df, config)\n",
    "        \n",
    "        # save\n",
    "        processed_dir = config[\"directories\"][\"processed_dir\"]\n",
    "        cleaned_df.to_csv(os.path.join(processed_dir, \"immigration_data_clean.csv\"), index=False)\n",
    "        speeches_only.to_csv(os.path.join(processed_dir, \"immigration_speeches_clean.csv\"), index=False)\n",
    "        \n",
    "        # summary\n",
    "        print(f\"\\nCreated cleaned dataset with {len(cleaned_df)} records\")\n",
    "        print(f\"Created filtered dataset with {len(speeches_only)} actual speeches\")\n",
    "\n",
    "        # party distribution\n",
    "        if 'party' in speeches_only.columns:\n",
    "            party_counts = speeches_only['party'].value_counts()\n",
    "            print(\"\\nParty distribution in speeches:\")\n",
    "            print(party_counts)\n",
    "\n",
    "        # category distribution\n",
    "        category_counts = speeches_only['speech_category'].value_counts()\n",
    "        print(\"\\nSpeech category distribution:\")\n",
    "        print(category_counts)\n",
    "        \n",
    "        return (cleaned_df, speeches_only)\n",
    "    else:\n",
    "        print(\"No data to clean.\")\n",
    "        return (None, None)\n",
    "\n",
    "# Uncomment to run data processing\n",
    "cleaned_df, speeches_only = run_data_processing(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file: processed_data/immigration_speeches.csv\n",
      "Processing 1785 rows...\n",
      "Cleaning full_text column...\n",
      "Saving cleaned data to: processed_data/immigration_speeches_cleaned.csv\n",
      "\n",
      "Sample of cleaned text:\n",
      "Row 1 (first 100 chars): [Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)] [Daily Digest] [Pages D909...\n",
      "Row 2 (first 100 chars): [Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)] [Extensions of Remarks] [P...\n",
      "\n",
      "Successfully processed 1785 rows.\n",
      "Cleaned CSV saved to: processed_data/immigration_speeches_cleaned.csv\n",
      "Reading CSV file: processed_data/immigration_data_clean.csv\n",
      "Processing 1785 rows...\n",
      "Cleaning full_text column...\n",
      "Saving cleaned data to: processed_data/immigration_data_clean_cleaned.csv\n",
      "\n",
      "Sample of cleaned text:\n",
      "Row 1 (first 100 chars): [Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)] [Daily Digest] [Pages D909...\n",
      "Row 2 (first 100 chars): [Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)] [Extensions of Remarks] [P...\n",
      "\n",
      "Successfully processed 1785 rows.\n",
      "Cleaned CSV saved to: processed_data/immigration_data_clean_cleaned.csv\n",
      "Reading CSV file: processed_data/immigration_speeches_clean.csv\n",
      "Processing 1593 rows...\n",
      "Cleaning full_text column...\n",
      "Saving cleaned data to: processed_data/immigration_speeches_clean_cleaned.csv\n",
      "\n",
      "Sample of cleaned text:\n",
      "Row 1 (first 100 chars): [Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)] [Extensions of Remarks] [P...\n",
      "Row 2 (first 100 chars): [Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)] [Extensions of Remarks] [P...\n",
      "\n",
      "Successfully processed 1593 rows.\n",
      "Cleaned CSV saved to: processed_data/immigration_speeches_clean_cleaned.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'processed_data/immigration_speeches_clean_cleaned.csv'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the full_text column in a CSV file to normalize whitespace\n",
    "def clean_whitespace_in_csv(input_file, output_file=None):\n",
    "\n",
    "    # determine output filename if not provided\n",
    "    if output_file is None:\n",
    "        base, ext = os.path.splitext(input_file)\n",
    "        output_file = f\"{base}_cleaned{ext}\"\n",
    "    \n",
    "    print(f\"Reading CSV file: {input_file}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_file, low_memory=False)\n",
    "        if 'full_text' not in df.columns:\n",
    "            print(\"Warning: 'full_text' column not found in CSV. Available columns:\")\n",
    "            print(\", \".join(df.columns))\n",
    "            return None\n",
    "        \n",
    "        total_rows = len(df)\n",
    "        print(f\"Processing {total_rows} rows...\")\n",
    "        \n",
    "        def clean_text(text):\n",
    "            if pd.isna(text):\n",
    "                return text\n",
    "                \n",
    "            cleaned = re.sub(r'\\s+', ' ', str(text))\n",
    "            cleaned = re.sub(r'\\n\\s*\\n', '\\n', cleaned)\n",
    "            cleaned = cleaned.strip()\n",
    "            return cleaned\n",
    "        \n",
    "        print(\"Cleaning full_text column...\")\n",
    "        df['full_text'] = df['full_text'].apply(clean_text)\n",
    "        \n",
    "        print(f\"Saving cleaned data to: {output_file}\")\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # print sample rows for verification\n",
    "        print(\"\\nSample of cleaned text:\")\n",
    "        for i, row in df.head(2).iterrows():\n",
    "            print(f\"Row {i+1} (first 100 chars): {row['full_text'][:100]}...\")\n",
    "        \n",
    "        print(f\"\\nSuccessfully processed {total_rows} rows.\")\n",
    "        print(f\"Cleaned CSV saved to: {output_file}\")\n",
    "        \n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "# example usage:\n",
    "clean_whitespace_in_csv('processed_data/immigration_speeches.csv')\n",
    "clean_whitespace_in_csv('processed_data/immigration_data_clean.csv')\n",
    "clean_whitespace_in_csv('processed_data/immigration_speeches_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. `immigration_speeches.csv`:\n",
    "   - The raw parsed data from your Congressional Record HTML files\n",
    "   - Contains all the immigration-related speeches and procedural text\n",
    "   - Includes metadata like date, speaker, chamber, etc., along with the full text extracted from HTML files\n",
    "   - This is the initial dataset created by the `process_immigration_files` function\n",
    "\n",
    "2. `immigration_data_clean.csv`:\n",
    "   - Contains all records (both speeches and procedural text) with cleaned and enhanced data\n",
    "   - Includes additional columns like standardized dates, party information, speech categorization\n",
    "   - Adds summary text and metrics like token count and sentence count\n",
    "   - This is the complete dataset after basic preprocessing\n",
    "\n",
    "3. `immigration_speeches_clean.csv`:\n",
    "   - A filtered subset of `immigration_data_clean.csv` containing only actual speeches (no procedural text)\n",
    "   - Uses the `is_speech` flag to filter out non-speech content\n",
    "   - This is the dataset you'd use for analyzing actual Congressional speeches about immigration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SECTION 4: LINGUISTIC TEXT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)]\n",
      "[Extensions of Remarks]\n",
      "[Page E1151]\n",
      "From the Congressional Record Online through the Government Publishing Office [www.gpo.gov]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " HONORING THE 40TH ANNIVERSARY OF SERVICE BY REVEREND DR. AMOS C. BROWN\n",
      "\n",
      "                                 ______\n",
      "                                 \n",
      "\n",
      "                           HON. NANCY PELOSI\n",
      "\n",
      "                             of california\n",
      "\n",
      "                    in the house of representatives\n",
      "\n",
      "                       Friday, September 1, 2017\n",
      "\n",
      "  Ms. PELOSI. Mr. Speaker, I rise with pride today to join my beloved \n",
      "city in recognizing the 40th Anniversary of Reverend Dr. Amos C. Brown \n",
      "as the pastor of the Third Baptist Church of San Francisco and honoring \n",
      "his lifetime of moral leadership, political activism and spiritual \n",
      "guidance.\n",
      "  Born and raised in Jackson, Mississippi, Reverend Brown's history of \n",
      "activism and his deep involvement in the civil rights movement began at \n",
      "an early age. He was only a teenager when he assumed leadership of his \n",
      "local NAACP chapter and was driven cross-country to San Francisco by \n",
      "activist Medgar Evers to meet the Reverend Martin Luther King Jr. \n",
      "Later, Brown attended Morehouse College where he was hand-selected by \n",
      "King as one of his eight students, inspiring him to seek a lifetime of \n",
      "service on behalf of civil rights and social justice. He was arrested \n",
      "together with Dr. King during a lunch counter sit-in Atlanta department \n",
      "store, and became one of the fearless Freedom Riders in 1961.\n",
      "  Brown received both a Mastery of Divinity from Crozer Theological \n",
      "Seminary and a Doctor of Ministry from United Theological Seminary. Our \n",
      "city was fortunate to welcome the Pastor and his beloved wife Jane \n",
      "Smith Brown in 1976, when they made San Francisco their home and the \n",
      "Third Baptist Church their place of worship and leadership.\n",
      "  Brown's vision of the church as a holy place of prayer, and also a \n",
      "place of civic engagement and social action, transformed the Third \n",
      "Baptist Church into a congregation at the forefront of the fight for \n",
      "civil rights. His unwavering belief in future generations drove the \n",
      "church to establish a summer school program, an after-school academic \n",
      "enrichment program, ``Back on Track'', and the Charles A. Tindley \n",
      "Academy of Music.\n",
      "  Brown's influence also extended beyond his local community when the \n",
      "church sponsored 80 children from Tanzania to receive heart surgery in \n",
      "the United States, and his congregation sponsored more African refugees \n",
      "than any other local congregation in the nation. This global reach and \n",
      "visionary leadership was recognized when Brown was appointed as a \n",
      "delegate to the 2001 United Nations Conference on Race and Intolerance \n",
      "in Durban, South Africa, representing the National Board of the NAACP.\n",
      "  Closer to home, Brown was awarded the Martin Luther King Jr. \n",
      "Ministerial Award for outstanding leadership and contributions to the \n",
      "Black Church in America and he was inducted into the International Hall \n",
      "of Fame of the King International Chapel at Morehouse College. He has \n",
      "served as the president of the NAACP Branch in San Francisco, as a San \n",
      "Francisco Community College Trustee and as a San Francisco County \n",
      "Supervisor.\n",
      "  A passionate fighter and voice for the most vulnerable in our \n",
      "community, Reverend Amos Brown has served as a source of change, \n",
      "strength and faith for 40 years. I congratulate him, along with his \n",
      "wife Jane and his children Amos C. Brown Jr., David Josephus and Kizzie \n",
      "Marie, on this milestone celebration and extend my best wishes as he \n",
      "continues to lead and inspire our community.\n",
      "\n",
      "                          ____________________\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explore and visualize the data to get a small understanding of it\n",
    "df = pd.read_csv(\"processed_data\\\\immigration_speeches_clean.csv\")\n",
    "first_row = df.iloc[0]\n",
    "print(first_row[\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1593\n",
      "Party value counts:\n",
      "party\n",
      "Democrat       577\n",
      "NaN            504\n",
      "Republican     496\n",
      "Independent     16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Example Democratic rows:\n",
      "                                           full_text\n",
      "0  [Congressional Record Volume 163, Number 141 (...\n",
      "1  [Congressional Record Volume 163, Number 141 (...\n",
      "2  [Congressional Record Volume 163, Number 141 (...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"processed_data/no_whitespace/immigration_speeches_clean_cleaned.csv\")\n",
    "\n",
    "print(\"Number of rows:\", len(df))\n",
    "print(\"Party value counts:\")\n",
    "print(df[\"party\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nExample Democratic rows:\")\n",
    "print(df[df[\"party\"] == \"Democrat\"][[\"full_text\"]].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Proposed analysis pipeline\n",
    "\n",
    "1. Term Frequency (tf-df) Analysis (RQ1)\n",
    "- Use the cleaned speeches dataset (immigration_speeches_clean.csv)\n",
    "- Calculate relative frequencies of key immigration terms for each party\n",
    "- Perform chi-square tests to determine statistical significance of term usage differences\n",
    "\n",
    "2. Contextual Analysis (TBD) (RQ3)\n",
    "- Extract 5-word windows around key terms\n",
    "- Use TF-IDF to identify distinctive contextual words for each party\n",
    "- Conduct collocation analysis to measure significant word co-occurrences\n",
    "\n",
    "3. Temporal Trend Analysis (RQ2)\n",
    "- Analyze terminology usage across different time periods (2018-2021)\n",
    "- Track shifts in terminology for each party over time\n",
    "- Visualize changes using time series plots\n",
    "\n",
    "4. Party Comparison (RQ1)\n",
    "- Compare terminology usage between:\n",
    "  - Democrats vs. Republicans\n",
    "  - Border state representatives vs. non-border state representatives\n",
    "  - Senators vs. Representatives\n",
    "\n",
    "5. Advanced Text Analysis Techniques (RQ3)\n",
    "- Topic modeling (LDA) to identify immigration-related speech topics\n",
    "- Analyze topic distribution between parties\n",
    "- Track topic evolution over time\n",
    "\n",
    "6. Statistical Validation (Cross-RQ Support)\n",
    "- Chi-square tests for word usage differences\n",
    "- Correlation analysis between terminology and voting patterns\n",
    "- Time series analysis of terminology shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Term Frequency (tf-df) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency Analysis Summary:\n",
      "\n",
      "Top Statistically Significant Terms:\n",
      "                 Democratic  Republican  Chi2_Statistic   P_Value\n",
      "cbp                     104         521       10.393805  0.001264\n",
      "dreamer                 512         795        9.533691  0.002017\n",
      "refugee                 323         163        7.673914  0.005602\n",
      "border                 1278        2669        7.121046  0.007618\n",
      "immigration             686        2525        7.042668  0.007959\n",
      "border security         375         543        5.414606  0.019969\n",
      "ice                     110         222        4.668733  0.030717\n",
      "                 Democratic  Republican  Chi2_Statistic   P_Value\n",
      "immigration             686        2525        7.042668  0.007959\n",
      "border                 1278        2669        7.121046  0.007618\n",
      "refugee                 323         163        7.673914  0.005602\n",
      "dreamer                 512         795        9.533691  0.002017\n",
      "border security         375         543        5.414606  0.019969\n",
      "ice                     110         222        4.668733  0.030717\n",
      "cbp                     104         521       10.393805  0.001264\n"
     ]
    }
   ],
   "source": [
    "# count occurrences of each term using provided regex patterns\n",
    "def count_term_occurrences(text, term_patterns):\n",
    "    term_counts = {}\n",
    "    for term, pattern in term_patterns.items():\n",
    "        term_counts[term] = len(re.findall(pattern, str(text), re.IGNORECASE))\n",
    "    return term_counts\n",
    "\n",
    "# analyze term frequency by party using regex-based term patterns\n",
    "def analyze_party_term_frequency(speeches_df, term_patterns):\n",
    "    terms = list(term_patterns.keys())\n",
    "    \n",
    "    # add precomputed matches to the DataFrame for all speeches\n",
    "    speeches_df['term_hits'] = speeches_df['full_text'].apply(lambda text: count_term_occurrences(text, term_patterns))\n",
    "    # normalize party labels\n",
    "    speeches_df['party'] = speeches_df['party'].replace({\n",
    "        'Democrat': 'Democratic',\n",
    "        'Republican': 'Republican',\n",
    "        'Independent': 'Independent'\n",
    "    })\n",
    "\n",
    "    # Initialize totals\n",
    "    party_term_counts = {\n",
    "        'Democratic': {term: 0 for term in terms},\n",
    "        'Republican': {term: 0 for term in terms}\n",
    "    }\n",
    "    party_speech_counts = {'Democratic': 0, 'Republican': 0}\n",
    "\n",
    "    # aggregate counts\n",
    "    for _, row in speeches_df.iterrows():\n",
    "        party = row['party']\n",
    "        if party in party_term_counts:\n",
    "            party_speech_counts[party] += 1\n",
    "            for term in terms:\n",
    "                party_term_counts[party][term] += row['term_hits'][term]\n",
    "\n",
    "    term_freq_df = pd.DataFrame(party_term_counts).fillna(0)\n",
    "\n",
    "    # chi-square calculations\n",
    "    chi_square_results = {}\n",
    "    for term in terms:\n",
    "        \n",
    "        dem_with_term = sum(1 for _, row in speeches_df[speeches_df['party'] == 'Democratic'].iterrows()\n",
    "                            if row['term_hits'][term] > 0)\n",
    "        rep_with_term = sum(1 for _, row in speeches_df[speeches_df['party'] == 'Republican'].iterrows()\n",
    "                            if row['term_hits'][term] > 0)\n",
    "\n",
    "        if (dem_with_term + rep_with_term) < 5:\n",
    "            chi_square_results[term] = {\n",
    "                'chi2': None, 'p_value': None, 'significant': False\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        contingency_table = np.array([\n",
    "            [dem_with_term, party_speech_counts['Democratic'] - dem_with_term],\n",
    "            [rep_with_term, party_speech_counts['Republican'] - rep_with_term]\n",
    "        ])\n",
    "\n",
    "        try:\n",
    "            chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "            chi_square_results[term] = {\n",
    "                'chi2': chi2,\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05  # Use 0.05 for strict, 0.10 for exploratory\n",
    "            }\n",
    "        except ValueError:\n",
    "            chi_square_results[term] = {\n",
    "                'chi2': None,\n",
    "                'p_value': None,\n",
    "                'significant': False\n",
    "            }\n",
    "\n",
    "    # additional metrics\n",
    "    term_freq_df['Total'] = term_freq_df['Democratic'] + term_freq_df['Republican']\n",
    "    term_freq_df['Dem_Proportion'] = term_freq_df['Democratic'] / term_freq_df['Total']\n",
    "    term_freq_df['Rep_Proportion'] = term_freq_df['Republican'] / term_freq_df['Total']\n",
    "    term_freq_df['Chi2_Statistic'] = [chi_square_results[term]['chi2'] for term in terms]\n",
    "    term_freq_df['P_Value'] = [chi_square_results[term]['p_value'] for term in terms]\n",
    "    term_freq_df['Statistically_Significant'] = [chi_square_results[term]['significant'] for term in terms]\n",
    "\n",
    "    return term_freq_df, chi_square_results\n",
    "\n",
    "\n",
    "#  create separate visualizations of term frequency and proportions\n",
    "def visualize_term_frequency(term_freq_df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    plot_df = term_freq_df.reset_index()\n",
    "    party_cols = [col for col in ['Democratic', 'Republican', 'Independent'] if col in term_freq_df.columns]\n",
    "\n",
    "    freq_df = plot_df.melt(id_vars=['index'], value_vars=party_cols,\n",
    "                           var_name='Party', value_name='Count')\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.barplot(x='index', y='Count', hue='Party', data=freq_df)\n",
    "    plt.title('Immigration Term Frequency by Party')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('term_frequency_by_party.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # prop. bar plot (if Total > 0 to avoid divide-by-zero)\n",
    "    proportion_df = plot_df.copy()\n",
    "    for party in party_cols:\n",
    "        proportion_df[f'{party}_Proportion'] = proportion_df[party] / proportion_df[party_cols].sum(axis=1)\n",
    "\n",
    "    proportion_df = proportion_df.melt(id_vars='index',\n",
    "        value_vars=[f'{p}_Proportion' for p in party_cols],\n",
    "        var_name='Party', value_name='Proportion')\n",
    "\n",
    "    proportion_df['Party'] = proportion_df['Party'].str.replace('_Proportion', '')\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.barplot(x='index', y='Proportion', hue='Party', data=proportion_df)\n",
    "    plt.title('Proportional Term Usage by Party')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('proportional_term_usage.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def run_term_frequency_analysis(config):\n",
    "    speeches_path = config['directories']['processed_dir'] + '/no_whitespace/immigration_speeches_clean_cleaned.csv'\n",
    "    speeches_df = pd.read_csv(speeches_path)\n",
    "\n",
    "    term_patterns = config['constants']['immigration_terms']\n",
    "\n",
    "    term_freq_df, chi_square_results = analyze_party_term_frequency(speeches_df, term_patterns)\n",
    "    visualize_term_frequency(term_freq_df)\n",
    "\n",
    "    term_freq_df.to_csv(config['directories']['processed_dir'] + '/party_term_frequency.csv', index=True)\n",
    "\n",
    "    print(\"Term Frequency Analysis Summary:\")\n",
    "    print(\"\\nTop Statistically Significant Terms:\")\n",
    "    print(term_freq_df[term_freq_df['Statistically_Significant']]\n",
    "        .sort_values('P_Value')[['Democratic', 'Republican', 'Chi2_Statistic', 'P_Value']]\n",
    "        .head(10))\n",
    "    significant_terms = term_freq_df[term_freq_df['Statistically_Significant']]\n",
    "    print(significant_terms[['Democratic', 'Republican', 'Chi2_Statistic', 'P_Value']])\n",
    "\n",
    "    return term_freq_df, chi_square_results\n",
    "\n",
    "# when ready to run uncomment\n",
    "results_df, chi_square_results = run_term_frequency_analysis(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1. Partisan Differences in Immigration Terminology Are Statistically Significant and Reveal Unexpected Patterns (RQ1)\n",
    "\n",
    "| Term         | Dem % | Rep % | p-value | Interpretation |\n",
    "|--------------|--------|--------|----------|----------------|\n",
    "| **immigration** | 33.7% | 66.3% | 0.0268 | Republicans use \"immigration\" significantly more frequently |\n",
    "| **dreamer**     | 31.1% | 68.9% | 0.0023 | Term strongly associated with Republican discourse, possibly surprising given its pro-immigrant connotation |\n",
    "| **wall**        | 40.1% | 59.9% | 0.0885 | Border wall-related rhetoric is more common among Republicans |\n",
    "| **ice**         | 42.5% | 57.5% | 0.0596 | Republican speeches mention ICE significantly more often |\n",
    "\n",
    "#### Key Takeaways:\n",
    "\n",
    "- **Republicans dominate immigration-related rhetoric**, especially with broadly used umbrella terms like *“immigration”* and *“dreamer.”*\n",
    "- Despite expectations, even terms like **\"dreamer\"**, often seen as sympathetic, appear more in **Republican** discourse — potentially due to **criticisms or calls for reform**.\n",
    "- Democrats do not dominate any terms **statistically**, but they **lean higher** in *“detention”* (66.9%) and *“cbp”* (62%) usage — likely in **critical or oversight contexts**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Temporal Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Compute average normalized frequency (per 1,000 tokens) by year/party/term\n",
    "def count_yearly_party_term_frequency_normalized(df, term_patterns):\n",
    "    df = df.copy()\n",
    "    df['year'] = pd.to_datetime(df['date_standard']).dt.year\n",
    "\n",
    "    # normalize party labels\n",
    "    df['party'] = df['party'].replace({\n",
    "        'Democrat': 'Democratic',\n",
    "        'Republican': 'Republican'\n",
    "    })\n",
    "\n",
    "    # count term matches in each full_text\n",
    "    df['term_hits'] = df['full_text'].apply(lambda text: count_term_occurrences(text, term_patterns))\n",
    "\n",
    "    # flatten structure and normalize by token count\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        token_count = row.get('token_count', 1)\n",
    "        token_count = token_count if pd.notna(token_count) and token_count > 0 else 1\n",
    "        for term, raw_count in row['term_hits'].items():\n",
    "            normalized = (raw_count / token_count) * 1000  # per 1,000 tokens\n",
    "            rows.append({\n",
    "                'year': row['year'],\n",
    "                'party': row['party'],\n",
    "                'term': term,\n",
    "                'count_per_1000': normalized\n",
    "            })\n",
    "\n",
    "    norm_df = pd.DataFrame(rows)\n",
    "\n",
    "    # aggregate by year, party, and term (average across speeches)\n",
    "    grouped = norm_df.groupby(['year', 'party', 'term'])['count_per_1000'].mean().reset_index()\n",
    "    return grouped\n",
    "\n",
    "# Step 2: Plot usage trend over time with annotations for events\n",
    "def plot_term_trends_over_time(grouped_df, highlight_events=True):\n",
    "    terms = grouped_df['term'].unique()\n",
    "\n",
    "    for term in terms:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        term_df = grouped_df[grouped_df['term'] == term]\n",
    "        sns.lineplot(data=term_df, x='year', y='count_per_1000', hue='party', marker='o')\n",
    "\n",
    "        plt.title(f\"Normalized Usage Over Time: '{term}'\")\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(\"Mentions per 1,000 Tokens\")\n",
    "        plt.xticks(sorted(grouped_df['year'].unique()))\n",
    "        plt.legend(title='Party')\n",
    "\n",
    "        if highlight_events:\n",
    "            for year, label in [\n",
    "                (2018, 'Family Separation'),\n",
    "                (2020, 'Election Year'),\n",
    "                (2021, 'Biden Inauguration'),\n",
    "                (2023, 'End of Title 42')\n",
    "            ]:\n",
    "                plt.axvline(x=year, linestyle='--', color='gray', alpha=0.6)\n",
    "                plt.text(year + 0.1, plt.ylim()[1] * 0.8, label, rotation=90, fontsize=9, alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'term_trend_{term}.png')\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df = pd.read_csv(config['directories']['processed_dir'] + '/no_whitespace/immigration_speeches_clean_cleaned.csv')\n",
    "term_patterns = config['constants']['immigration_terms']\n",
    "\n",
    "# run analysis and plot\n",
    "grouped_df = count_yearly_party_term_frequency_normalized(speeches_df, term_patterns)\n",
    "grouped_df.to_csv(\"normalized_term_trends_by_year.csv\", index=False)\n",
    "plot_term_trends_over_time(grouped_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to visualize trends for top 5-6 terms with clear party divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved individual trend plots to: topterm_trend_plots\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"results/RQ2/normalized_term_trends_by_year.csv\")\n",
    "\n",
    "# filter to major parties\n",
    "df = df[df[\"party\"].isin([\"Democratic\", \"Republican\"])]\n",
    "\n",
    "# compute top 6 divergent terms\n",
    "pivot = df.pivot_table(index=[\"year\", \"term\"], columns=\"party\", values=\"count_per_1000\").fillna(0)\n",
    "pivot[\"abs_diff\"] = (pivot[\"Democratic\"] - pivot[\"Republican\"]).abs()\n",
    "top_terms = pivot.groupby(\"term\")[\"abs_diff\"].mean().sort_values(ascending=False).head(6).index.tolist()\n",
    "\n",
    "output_dir = \"topterm_trend_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# one PNG per term\n",
    "for term in top_terms:\n",
    "    term_df = df[df[\"term\"] == term]\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.lineplot(data=term_df, x=\"year\", y=\"count_per_1000\", hue=\"party\", marker=\"o\")\n",
    "\n",
    "    # event annotations\n",
    "    for year, label in [(2018, 'Family Separation'), (2020, 'Election'), (2021, 'Biden Inauguration')]:\n",
    "        plt.axvline(x=year, linestyle='--', color='gray', alpha=0.5)\n",
    "        plt.text(year + 0.1, plt.ylim()[1] * 0.85, label, rotation=90, fontsize=8, alpha=0.7)\n",
    "\n",
    "    plt.title(f\"Usage Over Time: '{term}'\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Mentions per 1,000 Tokens\")\n",
    "    plt.legend(title=\"Party\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = os.path.join(output_dir, f\"{term.replace(' ', '_')}_trend.png\")\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Saved individual trend plots to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions from the results so far.\n",
    "\n",
    "1. Immigration is a Consistently Polarizing Topic\n",
    "\n",
    "    The top divergent terms highlight persistent framing differences, even across years with different administrations. This supports the hypothesis that immigration remains a highly partisan issue in U.S. politics.\n",
    "\n",
    "2. Political Events Drive Terminology Spikes\n",
    "\n",
    "    Event annotations (e.g. Family Separation, Elections) align with notable spikes in specific term frequencies. This validates your method and emphasizes the role of external events in shaping political discourse.\n",
    "\n",
    "3. Normalized Frequencies Reveal Subtle Trends\n",
    "\n",
    "    By measuring per 1,000 tokens, your analysis captures rhetorical shifts even when overall speech volume changes, allowing for fair year-to-year comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the visualizations of term usage from 2017–2021\n",
    "\n",
    "1. **“immigration”**\n",
    "- **Republican** usage peaked in 2018 (likely a response to the Family Separation policy), then dropped sharply through 2021.\n",
    "- **Democratic** usage declined steadily.\n",
    "- **Interpretation**: The term “immigration” became less central after 2018, possibly replaced by more specific framing (e.g., “migrant” or “border”) or due to fatigue and shifting focus during COVID.\n",
    "\n",
    "---\n",
    "\n",
    "2. **“migrant”**\n",
    "- Consistent **Democratic** dominance in usage, with a steady decline.\n",
    "- **Republican** usage was always lower and declined similarly.\n",
    "- **Interpretation**: “Migrant” is more common in humanitarian or individual-focused discourse, aligning with Democratic framing. Decreased usage over time could reflect reduced legislative attention or shifting terminology.\n",
    "\n",
    "---\n",
    "\n",
    "3. **“border”**\n",
    "- Sharp **Republican** spike in 2019, aligning with the *Wall* and *caravan crisis* narratives.\n",
    "- Both parties saw major drops post-2019.\n",
    "- **Interpretation**: “Border” surged with Trump-era policies and media focus but faded post-2020, possibly due to shifting public priorities and political strategy changes under Biden.\n",
    "\n",
    "---\n",
    "\n",
    "4. **“daca”**\n",
    "- Massive early **Democratic** emphasis (esp. 2017), tied to efforts to protect DACA recipients.\n",
    "- Near-zero usage from 2019 onward by both parties.\n",
    "- **Interpretation**: Declining mentions reflect stalled legislative movement and the issue becoming less central post-Trump.\n",
    "\n",
    "---\n",
    "\n",
    "5. **“mexico”**\n",
    "- Gradual **Republican** increase, overtaking Democrats by 2021.\n",
    "- 2020 spike from both parties, possibly linked to the “Remain in Mexico” policy or campaign rhetoric.\n",
    "- **Interpretation**: Republicans increasingly invoke “Mexico” in enforcement/geopolitical contexts; Democrats’ interest spiked in 2020 but declined post-election.\n",
    "\n",
    "---\n",
    "\n",
    "6. **“dreamer”**\n",
    "- Overwhelmingly a **Democratic** term in early years (2017–2018).\n",
    "- Usage collapsed by 2019.\n",
    "- **Interpretation**: “Dreamer” reflects advocacy for undocumented youth. Declining mentions may reflect reduced legislative hope or strategic deprioritization.\n",
    "\n",
    "---\n",
    "\n",
    " Cross-Cutting Themes & Final Takeaways\n",
    "\n",
    "1. **Term Selection Reflects Party Priorities**\n",
    "- Democrats gravitate toward *people-centered* terms (“dreamer”, “migrant”, “daca”).\n",
    "- Republicans lean into *security and enforcement* terms (“border”, “immigration”, “mexico”).\n",
    "\n",
    "2. **General Decline in Mentions**\n",
    "- Most terms declined after 2019, regardless of party.\n",
    "- This suggests:\n",
    "  - **Issue fatigue** or less policy activity.\n",
    "  - The dominance of **COVID-19** discourse starting in 2020.\n",
    "  - Fewer landmark immigration developments post-2019 until Title 42’s repeal.\n",
    "\n",
    "3. **Diminished Polarization by 2021**\n",
    "- In 2021, some terms converge in usage (e.g., “immigration” and “migrant”).\n",
    "- This may signal either bipartisan disengagement or a momentary cooling of public rhetoric under Biden.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-Depth Term Analysis\n",
    "\n",
    "1.. **“border”**\n",
    "Observed Trend:\n",
    "- Steady Republican usage in 2017–2018, with a **dramatic spike in 2019** (over 4 mentions per 1,000 tokens).\n",
    "- Democratic mentions also peaked in 2019 but at a lower level.\n",
    "- Sharp decline for both parties after 2019, stabilizing at low levels by 2021.\n",
    "\n",
    "Interpretation:\n",
    "This term's prominence in 2019 aligns with several key political developments:\n",
    "- **Trump's National Emergency Declaration (Feb 2019)** to fund the border wall likely fueled Republican talking points centered on “border security.”\n",
    "- **Caravan migration narratives** dominated media cycles, especially on conservative outlets, influencing speech patterns.\n",
    "- Democrats also ramped up border-related discussions during this time — often in opposition, emphasizing humanitarian concerns or the implications of militarizing the border.\n",
    "\n",
    "Party Framing:\n",
    "- **Republicans:** “Border” was used as a symbol of national security and legal enforcement.\n",
    "- **Democrats:** Framed around human impact, with critiques of detention centers and border patrol practices.\n",
    "\n",
    "---\n",
    "\n",
    "2. **“dreamer”**\n",
    "Observed Trend:\n",
    "- Extremely high usage by Democrats in 2017–2018 (peaking around 3.8–4.0 per 1,000 tokens).\n",
    "- Virtually disappears after 2019.\n",
    "- Republicans used this term only sparingly throughout.\n",
    "\n",
    "Interpretation:\n",
    "- The spike reflects intense Democratic advocacy following **Trump’s 2017 rescission of the DACA program**, which protected undocumented immigrants brought to the U.S. as children.\n",
    "- Democratic leaders pushed for the DREAM Act during 2017–2018, making “Dreamers” a centerpiece of their immigration messaging.\n",
    "- After **multiple failed legislative efforts** and **Supreme Court delays**, the term fell out of use, likely due to issue fatigue and legislative gridlock.\n",
    "\n",
    "Party Framing:\n",
    "- **Democrats:** Framed “dreamers” as blameless, high-achieving young people who deserved permanent protection.\n",
    "- **Republicans:** Rarely invoked the term, possibly due to its **sympathetic connotation**, instead framing immigration more broadly around legality and borders.\n",
    "\n",
    "---\n",
    "\n",
    "3. **“immigration”**\n",
    "Observed Trend:\n",
    "- **Republican** usage peaked in 2018 (2.0 per 1,000 tokens), coinciding with national debates on enforcement.\n",
    "- **Democratic** mentions steadily declined over the years.\n",
    "- By 2021, both parties nearly converged at minimal usage.\n",
    "\n",
    "Interpretation:\n",
    "- The **2018 surge** ties directly to the **Family Separation policy**, which became a flashpoint in immigration discourse.\n",
    "- Republican usage often reflected defense or justification of harsher enforcement under the Trump administration.\n",
    "- As **COVID-19** and **economic concerns** rose post-2020, immigration lost attention in congressional discourse — explaining the universal drop in usage.\n",
    "\n",
    "Party Framing:\n",
    "- **Republicans:** Focused on “immigration” as a security and law enforcement issue.\n",
    "- **Democrats:** Gradually de-emphasized the term itself in favor of more humanizing alternatives like “migrant,” “asylum,” or issue-specific terms like “DACA.”\n",
    "\n",
    "---\n",
    "\n",
    "Final Thoughts\n",
    "These terms capture broader partisan strategies:\n",
    "- Republicans centered enforcement and sovereignty (e.g. “border”).\n",
    "- Democrats emphasized vulnerable populations and individual stories (e.g. “dreamer”).\n",
    "\n",
    "Their rises and falls in frequency are tightly coupled to real-world policy fights, executive actions, and moments of national attention — underscoring how **language mirrors power, policy, and public pressure**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
