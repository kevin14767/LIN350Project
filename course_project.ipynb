{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIN350 Course Project - The Language of Immigration Politics: Terminology Differences Across Party Lines in Congressional Speeches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way I usually run jupyter notebooks is opening the anaconda prompt terminal and running the command *jupyter notebook* from there I go to visual studio and click on select kernel -> existing jupyter server -> localhost or you can copy and paste the url of the tab that opened up with the *jupyter notebook* command and then click on python and that should be it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep track of the work we're doing together we can use a github repository to update changes and sync up our work. The usual workflow for this should be.\n",
    "1. Any changes you have in your laptop can be added to the repository with \"git add ./\" from the terminal the notebook is in\n",
    "2. After adding the files and changes you can use \"git commit -m 'message here'\" For the message make sure its in quotations and it can be anything\n",
    "3. After adding and commiting you can \"git push\" which pushes ur changes to the repository\n",
    "4. Let's say there's changes in the repository that are not in your laptop you can fetch them with \"git pull\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other setup you might need to do is set environement variables in local computer since we don't want to share that in the repository for privacy issues. So to do this you would run commands in your notebook to set it up. I'll show you\n",
    "1. running \"%env\" in a code block will show you all the environment variables in the jupyter environment\n",
    "2. to set up the enviroment variable for our project run the command \"%env API_KEY=apikeyfromourgoogledocs\"\n",
    "3. After that running the first cell of code will setup the api key to be used as API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congressional Record Data Collector - Very simple for now, simple text data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Xlsxwriter in c:\\users\\kevin\\anaconda3\\lib\\site-packages (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install Xlsxwriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SECTION 1: INTRODUCTION AND SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing project...\n",
      "\n",
      "Directory structure created:\n",
      "  - base_dir: c:\\Users\\Kevin\\Downloads\\LIN350Project\n",
      "  - data_dir: c:\\Users\\Kevin\\Downloads\\LIN350Project\\data\n",
      "  - raw_data_dir: c:\\Users\\Kevin\\Downloads\\LIN350Project\\data\\congressional_record\n",
      "  - processed_dir: c:\\Users\\Kevin\\Downloads\\LIN350Project\\processed_data\n",
      "  - samples_dir: c:\\Users\\Kevin\\Downloads\\LIN350Project\\processed_data\\speech_samples\n",
      "  - figures_dir: c:\\Users\\Kevin\\Downloads\\LIN350Project\\processed_data\\figures\n",
      "Warning: API_KEY environment variable not found.\n",
      "Please run '%env API_KEY=your_api_key' in a cell.\n",
      "Constants defined:\n",
      "  - Date ranges: 8 periods\n",
      "  - Term pairs: 11 pairs/groups\n",
      "  - Immigration terms: 26 terms\n",
      "\n",
      "Project initialization complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Research Questions:\n",
    "1. What statistically significant differences exist in the frequency of immigration-related \n",
    "   terminology (e.g., \"undocumented\" vs. \"illegal\") between political parties?\n",
    "2. How do these terminological choices correlate with specific policy positions or votes?\n",
    "3. Has the terminology used by each party shifted over the past decade (2015-2025), \n",
    "   and if so, in what direction?\n",
    "\n",
    "The project analyzes Congressional Record speeches to investigate how politicians from\n",
    "different parties use immigration-related terminology, following the methodologies\n",
    "covered in the LIN350 course.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import xlsxwriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "\n",
    "def setup_directories():\n",
    "    # create all necessary directories for the project. returns a dictionary of important paths.\n",
    "    base_dir = os.path.join(os.getcwd())\n",
    "    \n",
    "    # main data directories\n",
    "    data_dir = os.path.join(base_dir, \"data\")\n",
    "    raw_data_dir = os.path.join(data_dir, \"congressional_record\")\n",
    "    processed_dir = os.path.join(base_dir, \"processed_data\")\n",
    "    samples_dir = os.path.join(processed_dir, \"speech_samples\")\n",
    "    figures_dir = os.path.join(processed_dir, \"figures\")  \n",
    "    \n",
    "    # create all directories\n",
    "    for directory in [data_dir, raw_data_dir, processed_dir, samples_dir, figures_dir]:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # return dictionary of paths for easy reference\n",
    "    return {\n",
    "        \"base_dir\": base_dir,\n",
    "        \"data_dir\": data_dir,\n",
    "        \"raw_data_dir\": raw_data_dir,\n",
    "        \"processed_dir\": processed_dir,\n",
    "        \"samples_dir\": samples_dir,\n",
    "        \"figures_dir\": figures_dir\n",
    "    }\n",
    "\n",
    "def setup_api_key():\n",
    "    # set up the API key for accessing the Congress.gov API. returns the API key.\n",
    "\n",
    "    # uncomment and run this line to set the API key in the notebook environment\n",
    "    # %env API_KEY=your_api_key_here\n",
    "    \n",
    "    try:\n",
    "        API_KEY = os.environ.get(\"API_KEY\")\n",
    "        if not API_KEY:\n",
    "            print(\"Warning: API_KEY environment variable not found.\")\n",
    "            print(\"Please run '%env API_KEY=your_api_key' in a cell.\")\n",
    "            return None\n",
    "        return API_KEY\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing API key: {e}\")\n",
    "        return None\n",
    "\n",
    "# define constants for data collection\n",
    "def define_constants():   \n",
    "    date_ranges = [\n",
    "        # 2019 - Border wall government shutdown\n",
    "        (\"2019-01-01\", \"2019-01-31\"),\n",
    "        \n",
    "        # Government shutdown over border wall funding\n",
    "        (\"2018-12-15\", \"2018-12-31\"),\n",
    "\n",
    "        # DACA debates\n",
    "        (\"2017-09-01\", \"2017-10-15\"),\n",
    "        (\"2018-01-15\", \"2018-02-15\"),\n",
    "        \n",
    "        # Border surge discussions\n",
    "        (\"2019-03-01\", \"2019-04-15\"),\n",
    "        \n",
    "        # Election year immigration discussions\n",
    "        (\"2020-01-15\", \"2020-02-15\"),\n",
    "        (\"2020-09-01\", \"2020-10-15\"),\n",
    "        \n",
    "        # Biden administration policy changes\n",
    "        (\"2021-01-20\", \"2021-03-01\")\n",
    "    ]\n",
    "    \n",
    "    # immigration-related term pairs for analysis\n",
    "    term_pairs = [\n",
    "        (\"undocumented\", \"illegal\", \"unauthorized\"),  # Status descriptors\n",
    "        (\"asylum seeker\", \"refugee\", \"migrant\"),      # Migration categories\n",
    "        (\"border security\", \"border crisis\", \"border management\"),  # Border framing\n",
    "        (\"path to citizenship\", \"amnesty\"),           # Legal status solutions\n",
    "        (\"dreamers\", \"daca recipients\"),              # Youth beneficiaries \n",
    "        (\"family separation\", \"child detention\"),      # Child policy framing\n",
    "        (\"chain migration\", \"family reunification\"),  # Family immigration framing\n",
    "        (\"alien\", \"foreign national\", \"noncitizen\", \"undocumented\"),  # Legal designation terms\n",
    "        (\"deportation\", \"removal\"),                   # Enforcement terminology\n",
    "        (\"sanctuary cities\", \"non-cooperative jurisdictions\"),  # Local policy framing\n",
    "        (\"border wall\", \"border barrier\", \"border infrastructure\")  # Border infrastructure\n",
    "    ]\n",
    "    \n",
    "    # immigration-related terms with more precise matching\n",
    "    immigration_terms = {\n",
    "        # regular terms - can appear within other words\n",
    "        'immigration': r'immigration',\n",
    "        'immigrant': r'immigrant',\n",
    "        'migrant': r'migrant',\n",
    "        'citizenship': r'citizenship',\n",
    "        'deportation': r'deportation',\n",
    "        \n",
    "        # terms that need word boundary checks\n",
    "        'border': r'\\b(?:border|borders)\\b',\n",
    "        'asylum': r'\\basylum\\b',\n",
    "        'refugee': r'\\b(?:refugee|refugees)\\b',\n",
    "        'undocumented': r'\\bundocumented\\b',\n",
    "        'illegal alien': r'\\billegal\\s+alien',\n",
    "        'unauthorized': r'\\bunauthorized\\b',\n",
    "        'wall': r'\\bwall\\b',\n",
    "        'daca': r'\\bdaca\\b',\n",
    "        'dreamer': r'\\b(?:dreamer|dreamers)\\b',\n",
    "        'visa': r'\\bvisa\\b',\n",
    "        'detention': r'\\bdetention\\b',\n",
    "        \n",
    "        # phrases\n",
    "        'family separation': r'family\\s+separation',\n",
    "        'child detention': r'child\\s+detention',\n",
    "        'border security': r'border\\s+security',\n",
    "        'border crisis': r'border\\s+crisis',\n",
    "        'path to citizenship': r'path\\s+to\\s+citizenship',\n",
    "        'amnesty': r'\\bamnesty\\b',\n",
    "        'caravan': r'\\bcaravan\\b',\n",
    "        \n",
    "        # specific entities\n",
    "        'mexico': r'\\bmexico\\b',\n",
    "        'ice': r'\\b(?:ice|immigration and customs enforcement)\\b',  # Only match whole word \"ice\"\n",
    "        'cbp': r'\\b(?:cbp|customs and border protection)\\b'\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"date_ranges\": date_ranges,\n",
    "        \"term_pairs\": term_pairs,\n",
    "        \"immigration_terms\": immigration_terms,\n",
    "    }\n",
    "\n",
    "# initialize the project\n",
    "def initialize_project():\n",
    "   \n",
    "    print(\"Initializing project...\\n\")\n",
    "    \n",
    "    directories = setup_directories()\n",
    "    print(f\"Directory structure created:\")\n",
    "    for name, path in directories.items():\n",
    "        print(f\"  - {name}: {path}\")\n",
    "    \n",
    "    api_key = setup_api_key()\n",
    "    if api_key:\n",
    "        print(f\"API key configured\")\n",
    "    \n",
    "    constants = define_constants()\n",
    "    print(f\"Constants defined:\")\n",
    "    print(f\"  - Date ranges: {len(constants['date_ranges'])} periods\")\n",
    "    print(f\"  - Term pairs: {len(constants['term_pairs'])} pairs/groups\")\n",
    "    print(f\"  - Immigration terms: {len(constants['immigration_terms'])} terms\")\n",
    "    \n",
    "    config = {\n",
    "        \"directories\": directories,\n",
    "        \"api_key\": api_key,\n",
    "        \"constants\": constants\n",
    "    }\n",
    "    \n",
    "    print(\"\\nProject initialization complete!\")\n",
    "    return config\n",
    "\n",
    "# run initialization\n",
    "config = initialize_project()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SECTION 2: DATA COLLECTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API key with GovInfo API...\n",
      "Success! Your API key is valid for the GovInfo API.\n",
      "Status code: 200\n",
      "\n",
      "Available collections:\n",
      "- Congressional Bills\n",
      "- Congressional Bill Status\n",
      "- Congressional Bill Summaries\n",
      "- United States Budget\n",
      "- Congressional Calendars\n",
      "Will download Congressional Record data for 289 dates\n",
      "\n",
      "Data collection complete!\n",
      "Successfully downloaded data for 0 out of 289 dates\n",
      "Data saved to: c:\\Users\\Kevin\\Downloads\\LIN350Project\\data\\congressional_record\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# function to generate all dates in a given range\n",
    "def get_dates_in_range(start_date, end_date):\n",
    "    # start_date (str): Start date in format 'YYYY-MM-DD'\n",
    "    # end_date (str): End date in format 'YYYY-MM-DD'\n",
    "        \n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "    date_list = []\n",
    "    current = start\n",
    "    while current <= end:\n",
    "        date_list.append(current.strftime(\"%Y-%m-%d\"))\n",
    "        current += timedelta(days=1)\n",
    "    return date_list\n",
    "\n",
    "def verify_api_key(api_key):\n",
    "\n",
    "    test_url = \"https://api.govinfo.gov/collections\"\n",
    "    params = {\n",
    "        'api_key': api_key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"Testing API key with GovInfo API...\")\n",
    "        response = requests.get(test_url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"Success! Your API key is valid for the GovInfo API.\")\n",
    "            print(f\"Status code: {response.status_code}\")\n",
    "            \n",
    "            # show the first few collections to confirm we got real data\n",
    "            collections = response.json().get('collections', [])\n",
    "            if collections:\n",
    "                print(\"\\nAvailable collections:\")\n",
    "                for collection in collections[:5]:\n",
    "                    print(f\"- {collection.get('collectionName', 'Unknown')}\")\n",
    "            return True\n",
    "            \n",
    "        elif response.status_code == 401 or response.status_code == 403:\n",
    "            print(\"Authentication failed. Your API key appears to be invalid.\")\n",
    "            print(f\"Status code: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"Received unexpected status code: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while testing the API key: {e}\")\n",
    "        return False\n",
    "\n",
    "# function to get Congressional Record data using the GovInfo API\n",
    "def get_congressional_record(date, api_key, raw_data_dir):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        date (str): Date in format 'YYYY-MM-DD'\n",
    "        api_key (str): API key for the GovInfo API\n",
    "        raw_data_dir (str): Directory to save raw data  \n",
    "    \"\"\"\n",
    "    package_id = f\"CREC-{date}\"\n",
    "    package_url = f\"https://api.govinfo.gov/packages/{package_id}/summary\"\n",
    "    params = {\n",
    "        'api_key': api_key\n",
    "    }\n",
    "    try:\n",
    "        # check if the package exists\n",
    "        response = requests.get(package_url, params=params)\n",
    "    \n",
    "        # if package doesn't exist or other error\n",
    "        if response.status_code != 200:\n",
    "            print(f\"No Congressional Record available for {date} (Status: {response.status_code})\")\n",
    "            return False\n",
    "        \n",
    "        # save the package summary\n",
    "        with open(os.path.join(raw_data_dir, f\"{package_id}-summary.json\"), 'w') as f:\n",
    "            json.dump(response.json(), f)\n",
    "        \n",
    "        # get granules (speeches and entries) \n",
    "        granules_url = f\"https://api.govinfo.gov/packages/{package_id}/granules\"\n",
    "        granules_params = {\n",
    "            'api_key': api_key,\n",
    "            'offset': 0,\n",
    "            'pageSize': 100  # Max page size\n",
    "        }\n",
    "        \n",
    "        # get first page of granules\n",
    "        granules_response = requests.get(granules_url, params=granules_params)\n",
    "        \n",
    "        if granules_response.status_code != 200:\n",
    "            print(f\"Failed to get granules for {date} (Status: {granules_response.status_code})\")\n",
    "            return False\n",
    "            \n",
    "        # save the granules list\n",
    "        with open(os.path.join(raw_data_dir, f\"{package_id}-granules.json\"), 'w') as f:\n",
    "            json.dump(granules_response.json(), f)\n",
    "            \n",
    "        # download content for each granule\n",
    "        granules = granules_response.json().get('granules', [])\n",
    "        \n",
    "        for granule in granules:\n",
    "            granule_id = granule.get('granuleId')\n",
    "            \n",
    "            # skip if no granule ID\n",
    "            if not granule_id:\n",
    "                continue\n",
    "            \n",
    "            # get the HTML content\n",
    "            content_url = f\"https://api.govinfo.gov/packages/{package_id}/granules/{granule_id}/htm\"\n",
    "            content_response = requests.get(content_url, params=params)\n",
    "            \n",
    "            if content_response.status_code == 200:\n",
    "                # save the HTML content\n",
    "                with open(os.path.join(raw_data_dir, f\"{package_id}-{granule_id}.html\"), 'w', encoding='utf-8') as f:\n",
    "                    f.write(content_response.text)\n",
    "            \n",
    "            # respect rate limit\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        print(f\"Successfully downloaded Congressional Record for {date} ({len(granules)} granules)\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving data for {date}: {e}\")\n",
    "        return False\n",
    "\n",
    "# main function to download Congressional Record data\n",
    "def collect_congressional_data(config):\n",
    "    \"\"\"\n",
    "    Args: config (dict): Project configuration\n",
    "    Returns: int: Number of successfully downloaded dates\n",
    "    \"\"\"\n",
    "    api_key = config[\"api_key\"]\n",
    "    date_ranges = config[\"constants\"][\"date_ranges\"]\n",
    "    raw_data_dir = config[\"directories\"][\"raw_data_dir\"]\n",
    "    \n",
    "    if not verify_api_key(api_key):\n",
    "        print(\"Cannot proceed with data collection due to invalid API key.\")\n",
    "        return 0\n",
    "    \n",
    "    all_dates = []\n",
    "    \n",
    "    # generate all dates in the specified ranges\n",
    "    for start_date, end_date in date_ranges:\n",
    "        dates = get_dates_in_range(start_date, end_date)\n",
    "        all_dates.extend(dates)\n",
    "    \n",
    "    print(f\"Will download Congressional Record data for {len(all_dates)} dates\")\n",
    "    \n",
    "    # download data for each date, commented out because I we already collected the data\n",
    "    successful_downloads = 0\n",
    "    # for date in tqdm(all_dates, desc=\"Downloading Congressional Records\"):\n",
    "    #     success = get_congressional_record(date, api_key, raw_data_dir)\n",
    "    #     if success:\n",
    "    #         successful_downloads += 1\n",
    "        \n",
    "    #     # Wait between requests to avoid rate limiting\n",
    "    #     time.sleep(1)\n",
    "    \n",
    "    print(f\"\\nData collection complete!\")\n",
    "    print(f\"Successfully downloaded data for {successful_downloads} out of {len(all_dates)} dates\")\n",
    "    print(f\"Data saved to: {raw_data_dir}\")\n",
    "    \n",
    "    return successful_downloads\n",
    "\n",
    "# Uncomment to run data collection\n",
    "successful_downloads = collect_congressional_data(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 3463 records from 1030 unique legislators\n"
     ]
    }
   ],
   "source": [
    "# build a df of legislators from the @unitedstates Github data (2015-2025)\n",
    "current_file = \"data\\\\legislator_data\\\\unitedstates.github.io\\\\legislators-current.json\"\n",
    "historical_file = \"data\\\\legislator_data\\\\unitedstates.github.io\\\\legislators-historical.json\"\n",
    "def build_legislators_dataframe(current_file=current_file, \n",
    "                               historical_file=historical_file):\n",
    "   \n",
    "    with open(current_file, 'r') as f:\n",
    "        current = json.load(f)\n",
    "    \n",
    "    with open(historical_file, 'r') as f:\n",
    "        historical = json.load(f)\n",
    "    \n",
    "    all_legislators = current + historical\n",
    "    legislator_records = []\n",
    "    \n",
    "    study_start = datetime.strptime('2015-01-01', '%Y-%m-%d')\n",
    "    study_end = datetime.strptime('2025-12-31', '%Y-%m-%d')\n",
    "    \n",
    "    for legislator in all_legislators:\n",
    "        # legislator info\n",
    "        legislator_id = legislator.get('id', {}).get('bioguide', '')\n",
    "        first_name = legislator.get('name', {}).get('first', '')\n",
    "        last_name = legislator.get('name', {}).get('last', '')\n",
    "        \n",
    "        # process each term to see if any fall within our study period\n",
    "        for term in legislator.get('terms', []):\n",
    "            term_start = datetime.strptime(term.get('start', '1900-01-01'), '%Y-%m-%d')\n",
    "            term_end = datetime.strptime(term.get('end', '2100-01-01'), '%Y-%m-%d')\n",
    "            \n",
    "            # check if this term overlaps with our study period\n",
    "            if (term_start <= study_end and term_end >= study_start):\n",
    "                record = {\n",
    "                    'bioguide_id': legislator_id,\n",
    "                    'first_name': first_name,\n",
    "                    'last_name': last_name,\n",
    "                    'last_name_upper': last_name.upper(),  # For easier matching\n",
    "                    'full_name': legislator.get('name', {}).get('official_full', f\"{first_name} {last_name}\"),\n",
    "                    'state': term.get('state', ''),\n",
    "                    'party': term.get('party', ''),\n",
    "                    'type': term.get('type', ''),  # 'sen' or 'rep'\n",
    "                    'term_start': term.get('start', ''),\n",
    "                    'term_end': term.get('end', ''),\n",
    "                    'state_rank': term.get('state_rank', '')  # 'junior' or 'senior' for senators\n",
    "                }\n",
    "                legislator_records.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(legislator_records)\n",
    "    df.to_csv('legislators_2015_2025.csv', index=False)\n",
    "    print(f\"Created DataFrame with {len(df)} records from {len(set(df['bioguide_id']))} unique legislators\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "legislators_df = build_legislators_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract speaker information and determine party from Congressional Record text\n",
    "# returns party and details about how the match was made\n",
    "def get_party_from_speech(speech_text, legislators_df):\n",
    "    # regex\n",
    "    speaker_match = re.search(r'(?:Mr\\.|Mrs\\.|Ms\\.) ([A-Z]+)(?:\\s+of\\s+([A-Za-z]+))?', speech_text)\n",
    "    \n",
    "    if not speaker_match:\n",
    "        return None, \"No speaker pattern found\"\n",
    "    \n",
    "    last_name = speaker_match.group(1)\n",
    "    state_name = speaker_match.group(2)\n",
    "    \n",
    "    matches = legislators_df[legislators_df['last_name_upper'] == last_name]\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        return None, f\"No match found for {last_name}\"\n",
    "    \n",
    "    if len(matches) == 1:\n",
    "        # single match - straightforward case\n",
    "        return matches.iloc[0]['party'], \"Unique last name match\"\n",
    "    \n",
    "    # multiple matches - try to narrow down with state\n",
    "    if state_name:\n",
    "        # convert full state name to abbreviation\n",
    "        state_abbrev = state_name_to_abbrev(state_name)\n",
    "        state_matches = matches[matches['state'] == state_abbrev]\n",
    "        \n",
    "        if len(state_matches) == 1:\n",
    "            return state_matches.iloc[0]['party'], f\"Resolved with state ({state_abbrev})\"\n",
    "        elif len(state_matches) > 1:\n",
    "            # still multiple matches with same state\n",
    "            # sort by term_end to get the most recent/current legislator\n",
    "            recent_match = state_matches.sort_values('term_end', ascending=False).iloc[0]\n",
    "            return recent_match['party'], f\"Multiple matches with state, using most recent ({recent_match['full_name']})\"\n",
    "    \n",
    "    # no state or state didn't narrow it down - use most recent term\n",
    "    recent_match = matches.sort_values('term_end', ascending=False).iloc[0]\n",
    "    return recent_match['party'], f\"Multiple matches, using most recent ({recent_match['full_name']})\"\n",
    "\n",
    "# helper function to get state name to abbrev\n",
    "def state_name_to_abbrev(state_name):\n",
    "    states = {\n",
    "        'alabama': 'AL', 'alaska': 'AK', 'arizona': 'AZ', 'arkansas': 'AR', 'california': 'CA',\n",
    "        'colorado': 'CO', 'connecticut': 'CT', 'delaware': 'DE', 'florida': 'FL', 'georgia': 'GA',\n",
    "        'hawaii': 'HI', 'idaho': 'ID', 'illinois': 'IL', 'indiana': 'IN', 'iowa': 'IA',\n",
    "        'kansas': 'KS', 'kentucky': 'KY', 'louisiana': 'LA', 'maine': 'ME', 'maryland': 'MD',\n",
    "        'massachusetts': 'MA', 'michigan': 'MI', 'minnesota': 'MN', 'mississippi': 'MS', 'missouri': 'MO',\n",
    "        'montana': 'MT', 'nebraska': 'NE', 'nevada': 'NV', 'new hampshire': 'NH', 'new jersey': 'NJ',\n",
    "        'new mexico': 'NM', 'new york': 'NY', 'north carolina': 'NC', 'north dakota': 'ND', 'ohio': 'OH',\n",
    "        'oklahoma': 'OK', 'oregon': 'OR', 'pennsylvania': 'PA', 'rhode island': 'RI', 'south carolina': 'SC',\n",
    "        'south dakota': 'SD', 'tennessee': 'TN', 'texas': 'TX', 'utah': 'UT', 'vermont': 'VT',\n",
    "        'virginia': 'VA', 'washington': 'WA', 'west virginia': 'WV', 'wisconsin': 'WI', 'wyoming': 'WY'\n",
    "    }\n",
    "    \n",
    "    return states.get(state_name.lower(), state_name)\n",
    "\n",
    "# example usage:\n",
    "def analyze_speech_by_party(speech_text, legislators_df, term_pairs):\n",
    "    \"\"\"\n",
    "    Analyze usage of immigration term pairs in a speech text\n",
    "    and attribute them to the speaker's party\n",
    "    \"\"\"\n",
    "    party, match_details = get_party_from_speech(speech_text, legislators_df)\n",
    "    \n",
    "    if not party:\n",
    "        return None, f\"Could not determine party: {match_details}\"\n",
    "    \n",
    "    # init. dictionary to store term counts by party\n",
    "    term_counts = {term: 0 for pair in term_pairs for term in pair}\n",
    "    \n",
    "    # count occurrences of each term in the speech\n",
    "    for pair in term_pairs:\n",
    "        for term in pair:\n",
    "            # count how many times the term appears in the speech (case insensitive)\n",
    "            count = len(re.findall(r'\\b' + re.escape(term) + r'\\b', speech_text, re.IGNORECASE))\n",
    "            term_counts[term] = count\n",
    "    \n",
    "    return party, term_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SECTION 3: DATA PROCESSING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to identify immigration-related files\n",
    "def identify_immigration_files(config):\n",
    "    \"\"\"\n",
    "    searches through all HTML files and identifies those containing immigration-related terms.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Project configuration\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame of immigration-related files\n",
    "    \"\"\"\n",
    "    raw_data_dir = config[\"directories\"][\"raw_data_dir\"]\n",
    "    processed_dir = config[\"directories\"][\"processed_dir\"]\n",
    "    immigration_terms = config[\"constants\"][\"immigration_terms\"]\n",
    "    \n",
    "    # list of all HTML files\n",
    "    html_files = glob.glob(os.path.join(raw_data_dir, \"*.html\"))\n",
    "    total_files = len(html_files)\n",
    "    \n",
    "    print(f\"Found {total_files} HTML files in {raw_data_dir}\")\n",
    "    \n",
    "    # check the first few files to make sure we can access them\n",
    "    if total_files > 0:\n",
    "        print(\"\\nSample filenames:\")\n",
    "        for file in html_files[:5]:\n",
    "            print(f\"  - {os.path.basename(file)}\")\n",
    "        \n",
    "        # try to open one file to verify access\n",
    "        try:\n",
    "            with open(html_files[0], 'r', encoding='utf-8') as f:\n",
    "                first_chars = f.read(200)\n",
    "            print(\"\\nSuccessfully read first file. First 200 characters:\")\n",
    "            print(first_chars.replace('\\n', ' ')[:200])\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError reading file: {e}\")\n",
    "    \n",
    "    # search for immigration-related content\n",
    "    immigration_files = []\n",
    "    print(f\"Searching {total_files} files for immigration content...\")\n",
    "\n",
    "    for file in tqdm(html_files, desc=\"Searching files for immigration terms\"):\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                content = f.read().lower()\n",
    "                \n",
    "            # check each term with its specific regex pattern\n",
    "            found_terms = []\n",
    "            for term, pattern in immigration_terms.items():\n",
    "                if re.search(pattern, content):\n",
    "                    found_terms.append(term)\n",
    "            \n",
    "            if found_terms:\n",
    "                # extract date from filename\n",
    "                filename = os.path.basename(file)\n",
    "                date_parts = filename.split('-')\n",
    "                if len(date_parts) >= 2:\n",
    "                    date = date_parts[1]\n",
    "                else:\n",
    "                    date = \"Unknown\"\n",
    "                \n",
    "                immigration_files.append({\n",
    "                    'file': file,\n",
    "                    'date': date,\n",
    "                    'terms': ', '.join(found_terms)  # convert list to string\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {os.path.basename(file)}: {e}\")\n",
    "\n",
    "    # save results to CSV\n",
    "    if immigration_files:\n",
    "        immigration_df = pd.DataFrame(immigration_files)\n",
    "        csv_path = os.path.join(processed_dir, \"immigration_files.csv\")\n",
    "        immigration_df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        print(f\"\\nFound {len(immigration_files)} files with immigration content\")\n",
    "        print(f\"List saved to: {csv_path}\")\n",
    "        \n",
    "        # show sample of found files\n",
    "        print(\"\\nSample immigration-related files:\")\n",
    "        for file_info in immigration_files[:5]:\n",
    "            print(f\"  - {os.path.basename(file_info['file'])}: {file_info['terms']}\")\n",
    "    else:\n",
    "        print(\"No immigration-related files found.\")\n",
    "        immigration_df = pd.DataFrame()\n",
    "    \n",
    "    return immigration_df\n",
    "\n",
    "# function to extract structured data from HTML\n",
    "def parse_congressional_record(file_path):\n",
    "    \"\"\"\n",
    "    Parse a Congressional Record HTML file to extract structured data,\n",
    "    making better use of HTML structure and BeautifulSoup capabilities.\n",
    "   \n",
    "    Args:\n",
    "        file_path (str): Path to the HTML file\n",
    "       \n",
    "    Returns:\n",
    "        dict: Dictionary of extracted data, or None if parsing failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "       \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        # grab the pre element which contains all the content\n",
    "        pre_content = soup.find('pre')\n",
    "        if not pre_content:\n",
    "            return None\n",
    "            \n",
    "        # get the title from the <title> tag instead of regex if available\n",
    "        title_tag = soup.find('title')\n",
    "        page_title = title_tag.get_text() if title_tag else \"Unknown\"\n",
    "        \n",
    "        # extract the full text\n",
    "        full_text = pre_content.get_text()\n",
    "        \n",
    "        # parde header information and date from the congressional record header\n",
    "        header_text = pre_content.contents[0] if pre_content.contents else \"\"\n",
    "        date_match = re.search(r'\\[Congressional Record Volume \\d+, Number \\d+ \\(([^)]+)\\)\\]', str(header_text))\n",
    "        date = date_match.group(1) if date_match else \"Unknown\"\n",
    "        \n",
    "        # determine chamber from the header section (more reliable than searching the whole text)\n",
    "        chamber_lines = [line for line in str(header_text).split('\\n') if '[House]' in line or '[Senate]' in line]\n",
    "        chamber = \"House\" if chamber_lines and '[House]' in chamber_lines[0] else \\\n",
    "                 \"Senate\" if chamber_lines and '[Senate]' in chamber_lines[0] else \"Unknown\"\n",
    "        \n",
    "        # extract links if present, could be useful metadata\n",
    "        links = []\n",
    "        for a_tag in pre_content.find_all('a'):\n",
    "            href = a_tag.get('href', '')\n",
    "            text = a_tag.get_text()\n",
    "            links.append({\"href\": href, \"text\": text})\n",
    "        \n",
    "        # extract speaker information, look specifically for the parenthetical permission section\n",
    "        # regex find\n",
    "        speaker_section = re.search(r'\\(((?:Mr\\.|Mrs\\.|Ms\\.|Senator|Representative)\\s+[A-Z]+[^)]*)\\)', full_text)\n",
    "        \n",
    "        speaker_full = \"Unknown\"\n",
    "        speaker_last = \"Unknown\"\n",
    "        \n",
    "        if speaker_section:\n",
    "            speaker_text = speaker_section.group(1)\n",
    "            # extract the actual speaker name from this section\n",
    "            speaker_match = re.search(r'((?:Mr\\.|Mrs\\.|Ms\\.|Senator|Representative)\\s+([A-Z]+))', speaker_text)\n",
    "            if speaker_match:\n",
    "                speaker_full = speaker_match.group(1)\n",
    "                speaker_last = speaker_match.group(2)\n",
    "        \n",
    "        # extract title - look for content after the speaker declaration\n",
    "        # In Congressional Record format, typically the title/topic appears after the speaker is introduced\n",
    "        title = \"Unknown\"\n",
    "        title_section = re.search(r'to address the House[^.]*\\.\\)\\s+([A-Z][A-Z\\s\\'\",.()-]+?)\\s*\\n', full_text)\n",
    "        if title_section:\n",
    "            title = title_section.group(1).strip()\n",
    "        \n",
    "        # get granule ID from filename\n",
    "        filename = os.path.basename(file_path)\n",
    "        granule_id = filename.replace(\".html\", \"\")\n",
    "        \n",
    "        # extract page number which is often important for citation\n",
    "        page_match = re.search(r'\\[Page ([^\\]]+)\\]', full_text)\n",
    "        page_number = page_match.group(1) if page_match else \"Unknown\"\n",
    "        \n",
    "        return {\n",
    "            'file_id': granule_id,\n",
    "            'date': date,\n",
    "            'chamber': chamber,\n",
    "            'speaker_full': speaker_full,\n",
    "            'speaker_last': speaker_last,\n",
    "            'title': title,\n",
    "            'page_number': page_number,\n",
    "            'links': links,\n",
    "            'page_title': page_title,\n",
    "            'full_text': full_text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {os.path.basename(file_path)}: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process all immigration-related files\n",
    "def process_immigration_files(config, immigration_df=None):\n",
    "    \"\"\"\n",
    "    Process all immigration-related files to extract structured data.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Project configuration\n",
    "        immigration_df (pandas.DataFrame, optional): df of immigration-related files\n",
    "            If None, the function will try to load it from a file\n",
    "            \n",
    "    Returns:\n",
    "        pandas.DataFrame: df of processed immigration speeches\n",
    "    \"\"\"\n",
    "    processed_dir = config[\"directories\"][\"processed_dir\"]\n",
    "    \n",
    "    # if no df is provided, try to load it from a file\n",
    "    if immigration_df is None:\n",
    "        immigration_files_csv = os.path.join(processed_dir, \"immigration_files.csv\")\n",
    "        if not os.path.exists(immigration_files_csv):\n",
    "            print(f\"Error: Immigration files list not found at {immigration_files_csv}\")\n",
    "            return None\n",
    "        \n",
    "        immigration_df = pd.read_csv(immigration_files_csv)\n",
    "    \n",
    "    print(f\"Processing {len(immigration_df)} immigration-related files...\")\n",
    "    \n",
    "    # process each file in the immigration list\n",
    "    parsed_data = []\n",
    "    for _, row in tqdm(immigration_df.iterrows(), total=len(immigration_df), desc=\"Parsing HTML files\"):\n",
    "        file_path = row['file']\n",
    "        extracted_data = parse_congressional_record(file_path)\n",
    "        \n",
    "        if extracted_data:\n",
    "            # add the immigration terms found\n",
    "            extracted_data['immigration_terms'] = row['terms']\n",
    "            parsed_data.append(extracted_data)\n",
    "    \n",
    "    # create a df and save to CSV\n",
    "    if parsed_data:\n",
    "        parsed_df = pd.DataFrame(parsed_data)\n",
    "        csv_path = os.path.join(processed_dir, \"immigration_speeches.csv\")\n",
    "        parsed_df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        print(f\"\\nSuccessfully parsed {len(parsed_data)} files\")\n",
    "        print(f\"Data saved to: {csv_path}\")\n",
    "        \n",
    "        # print summary of speakers found\n",
    "        speaker_counts = parsed_df['speaker_last'].value_counts()\n",
    "        print(f\"\\nTop 10 speakers in the dataset:\")\n",
    "        print(speaker_counts.head(10))\n",
    "        \n",
    "        # print example of first record\n",
    "        print(\"\\nExample of parsed data (first record):\")\n",
    "        for key, value in parsed_data[0].items():\n",
    "            if key == 'full_text':\n",
    "                print(f\"{key}: {value[:200]}...\") # Print only first 200 chars of text\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(\"No data could be parsed from the files.\")\n",
    "        parsed_df = pd.DataFrame()\n",
    "    \n",
    "    return parsed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean and improve data\n",
    "def clean_data(df, config):\n",
    "    \"\"\"\n",
    "    Clean and enhance the parsed data.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame of parsed speeches\n",
    "        config (dict): Project configuration\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame of all cleaned records, DataFrame of actual speeches only)\n",
    "    \"\"\"\n",
    "    # create a copy to avoid modifying the original\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # 1. Convert dates to standard format\n",
    "    def standardize_date(date_str):\n",
    "        try:\n",
    "            if pd.isna(date_str) or date_str == \"Unknown\":\n",
    "                return None\n",
    "            # parse date string to datetime object\n",
    "            date_obj = datetime.strptime(date_str, \"%A, %B %d, %Y\")\n",
    "            # convert to standard format\n",
    "            return date_obj.strftime(\"%Y-%m-%d\")\n",
    "        except:\n",
    "            return date_str\n",
    "    \n",
    "    cleaned_df['date_standard'] = cleaned_df['date'].apply(standardize_date)\n",
    "    \n",
    "    # 2. Identify real speeches vs. procedural text\n",
    "    def is_real_speech(row):\n",
    "        # check if it's likely a speech by a member of Congress\n",
    "        # if speaker is Unknown, probably not a speech\n",
    "        if row['speaker_last'] == \"Unknown\":\n",
    "            return False\n",
    "        # check for procedural titles\n",
    "        procedural_titles = ['HOUSE', 'SENATE', 'PRAYER', 'PLEDGE', 'ADJOURNMENT', \n",
    "                            'RECESS', 'AMENDMENT', 'RECORD', 'MOTION', 'RESOLUTION']\n",
    "        if any(title in row['title'] for title in procedural_titles):\n",
    "            return False\n",
    "        # check for very short texts (likely not speeches)\n",
    "        if len(row['full_text']) < 500:\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    cleaned_df['is_speech'] = cleaned_df.apply(is_real_speech, axis=1)\n",
    "    \n",
    "    # 3. Categorize speech type\n",
    "    def categorize_speech(row):\n",
    "        text = row['full_text'].lower()\n",
    "        \n",
    "        if not row['is_speech']:\n",
    "            return \"procedural\"\n",
    "            \n",
    "        categories = {\n",
    "            \"border_security\": [\"border security\", \"border wall\", \"border crisis\"],\n",
    "            \"legal_status\": [\"undocumented\", \"illegal alien\", \"unauthorized\", \"amnesty\", \"path to citizenship\"],\n",
    "            \"children\": [\"daca\", \"dreamer\", \"child\", \"family separation\"],\n",
    "            \"asylum\": [\"asylum\", \"refugee\", \"humanitarian\"],\n",
    "            \"general\": [\"immigration\", \"immigrant\", \"migrant\"]\n",
    "        }\n",
    "        \n",
    "        for category, terms in categories.items():\n",
    "            if any(term in text for term in terms):\n",
    "                return category\n",
    "                \n",
    "        return \"other\"\n",
    "    \n",
    "    cleaned_df['speech_category'] = cleaned_df.apply(categorize_speech, axis=1)\n",
    "    \n",
    "    # 4. Extract a summary from the full text (first 300 characters)\n",
    "    def extract_summary(text):\n",
    "        # remove header content in square brackets\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        # remove whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # take first 300 characters\n",
    "        return text[:300] + \"...\" if len(text) > 300 else text\n",
    "    \n",
    "    cleaned_df['speech_summary'] = cleaned_df['full_text'].apply(extract_summary)\n",
    "    \n",
    "    # 5. add party information the legislator matching\n",
    "    def get_party_info(row):\n",
    "        \"\"\"Get party information for the speech using the speaker matching function\"\"\"\n",
    "        party, match_details = get_party_from_speech(row['full_text'], legislators_df)\n",
    "        return party if party else None\n",
    "    \n",
    "    cleaned_df['party'] = cleaned_df.apply(get_party_info, axis=1)\n",
    "    \n",
    "    # 6. Count tokens (words) in each speech\n",
    "    def count_tokens(text):\n",
    "        try:\n",
    "            # simple tokenization (split on whitespace)\n",
    "            return len(re.findall(r'\\b\\w+\\b', text))\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    cleaned_df['token_count'] = cleaned_df['full_text'].apply(count_tokens)\n",
    "    \n",
    "    # 7. Count sentences in each speech\n",
    "    def count_sentences(text):\n",
    "        try:\n",
    "            # simplistic sentence splitting (may not be perfect)\n",
    "            return len(re.findall(r'[.!?]+', text)) + 1\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    cleaned_df['sentence_count'] = cleaned_df['full_text'].apply(count_sentences)\n",
    "    \n",
    "    # 8. Add page number from the improved parser\n",
    "    if 'page_number' in df.columns:\n",
    "        cleaned_df['page_number'] = df['page_number']\n",
    "    \n",
    "    # 9. Extract links from the improved parser if available\n",
    "    if 'links' in df.columns:\n",
    "        cleaned_df['links'] = df['links']\n",
    "    \n",
    "    # 10. Keep only relevant columns in a useful order\n",
    "    columns_order = [\n",
    "        'file_id', 'date_standard', 'chamber', 'speaker_full', 'speaker_last', \n",
    "        'party', 'title', 'is_speech', 'speech_category', 'speech_summary', \n",
    "        'token_count', 'sentence_count', 'page_number', 'immigration_terms', 'full_text'\n",
    "    ]\n",
    "    \n",
    "    # filter columns that actually exist in the DataFrame\n",
    "    columns_order = [col for col in columns_order if col in cleaned_df.columns]\n",
    "    \n",
    "    # return only the columns we want and create filtered dataset with only actual speeches\n",
    "    cleaned_df = cleaned_df[columns_order]\n",
    "    speeches_only = cleaned_df[cleaned_df['is_speech'] == True]\n",
    "    \n",
    "    return (cleaned_df, speeches_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following runs the data processing pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14629 HTML files in c:\\Users\\Kevin\\Downloads\\LIN350Project\\data\\congressional_record\n",
      "\n",
      "Sample filenames:\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-2.html\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-3.html\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-4.html\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-5.html\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-6.html\n",
      "\n",
      "Successfully read first file. First 200 characters:\n",
      "<html> <head> <title>Congressional Record, Volume 163 Issue 141 (Friday, September 1, 2017)</title> </head> <body><pre> [Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)] [Daily\n",
      "Searching 14629 files for immigration content...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b157e6c5c5e404aa09f5fca374329ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Searching files for immigration terms:   0%|          | 0/14629 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1785 files with immigration content\n",
      "List saved to: c:\\Users\\Kevin\\Downloads\\LIN350Project\\processed_data\\immigration_files.csv\n",
      "\n",
      "Sample immigration-related files:\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-6.html: visa\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgE1151-4.html: refugee\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgE1152-3.html: immigration, immigrant, migrant, citizenship, deportation, undocumented, daca, dreamer, visa\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgE1154-4.html: undocumented, mexico\n",
      "  - CREC-2017-09-01-CREC-2017-09-01-pt1-PgH6632-6.html: mexico\n",
      "Processing 1785 immigration-related files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc85bf259d0a442195bf63c3c0edc8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing HTML files:   0%|          | 0/1785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully parsed 1785 files\n",
      "Data saved to: c:\\Users\\Kevin\\Downloads\\LIN350Project\\processed_data\\immigration_speeches.csv\n",
      "\n",
      "Top 10 speakers in the dataset:\n",
      "speaker_last\n",
      "Unknown    1145\n",
      "C            58\n",
      "S            54\n",
      "B            43\n",
      "R            42\n",
      "M            39\n",
      "H            36\n",
      "G            27\n",
      "T            26\n",
      "E            24\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Example of parsed data (first record):\n",
      "file_id: CREC-2017-09-01-CREC-2017-09-01-pt1-PgD909-6\n",
      "date: Friday, September 1, 2017\n",
      "chamber: Unknown\n",
      "speaker_full: Unknown\n",
      "speaker_last: Unknown\n",
      "title: Unknown\n",
      "page_number: D910\n",
      "links: [{'href': 'https://www.gpo.gov', 'text': 'www.gpo.gov'}, {'href': 'http://www.govinfo.gov', 'text': 'www.govinfo.gov'}, {'href': 'mailto:contactcenter@gpo.gov', 'text': 'contactcenter@gpo.gov'}]\n",
      "page_title: Congressional Record, Volume 163 Issue 141 (Friday, September 1, 2017)\n",
      "full_text: \n",
      "[Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)]\n",
      "[Daily Digest]\n",
      "[Pages D909-D910]\n",
      "From the Congressional Record Online through the Government Publishing Office [www.gpo.gov]\n",
      "...\n",
      "immigration_terms: visa\n",
      "\n",
      "Created cleaned dataset with 1785 records\n",
      "Created filtered dataset with 640 actual speeches\n",
      "\n",
      "Party distribution in speeches:\n",
      "party\n",
      "Democrat       199\n",
      "Republican     194\n",
      "Independent      3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Speech category distribution:\n",
      "speech_category\n",
      "children           266\n",
      "border_security    136\n",
      "other              119\n",
      "legal_status        58\n",
      "general             37\n",
      "asylum              24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# run the data processing pipeline\n",
    "def run_data_processing(config):\n",
    "    \"\"\"\n",
    "    Run the data processing pipeline, focusing only on preprocessing.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Project configuration\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (DataFrame of all cleaned records, DataFrame of speeches only)\n",
    "    \"\"\"\n",
    "    # Step 1: Identify immigration-related files\n",
    "    immigration_df = identify_immigration_files(config)\n",
    "    \n",
    "    # Step 2: Process immigration-related files\n",
    "    speeches_df = process_immigration_files(config, immigration_df)\n",
    "    \n",
    "    # Step 3: Clean and enhance the data\n",
    "    if speeches_df is not None and not speeches_df.empty:\n",
    "        cleaned_df, speeches_only = clean_data(speeches_df, config)\n",
    "        \n",
    "        # save\n",
    "        processed_dir = config[\"directories\"][\"processed_dir\"]\n",
    "        cleaned_df.to_csv(os.path.join(processed_dir, \"immigration_data_clean.csv\"), index=False)\n",
    "        speeches_only.to_csv(os.path.join(processed_dir, \"immigration_speeches_clean.csv\"), index=False)\n",
    "        \n",
    "        # summary\n",
    "        print(f\"\\nCreated cleaned dataset with {len(cleaned_df)} records\")\n",
    "        print(f\"Created filtered dataset with {len(speeches_only)} actual speeches\")\n",
    "\n",
    "        # party distribution\n",
    "        if 'party' in speeches_only.columns:\n",
    "            party_counts = speeches_only['party'].value_counts()\n",
    "            print(\"\\nParty distribution in speeches:\")\n",
    "            print(party_counts)\n",
    "\n",
    "        # category distribution\n",
    "        category_counts = speeches_only['speech_category'].value_counts()\n",
    "        print(\"\\nSpeech category distribution:\")\n",
    "        print(category_counts)\n",
    "        \n",
    "        return (cleaned_df, speeches_only)\n",
    "    else:\n",
    "        print(\"No data to clean.\")\n",
    "        return (None, None)\n",
    "\n",
    "# Uncomment to run data processing\n",
    "cleaned_df, speeches_only = run_data_processing(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file: processed_data/immigration_speeches.csv\n",
      "Processing 1785 rows...\n",
      "Cleaning full_text column...\n",
      "Saving cleaned data to: processed_data/immigration_speeches_cleaned.csv\n",
      "\n",
      "Sample of cleaned text:\n",
      "Row 1 (first 100 chars): [Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)] [Daily Digest] [Pages D909...\n",
      "Row 2 (first 100 chars): [Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)] [Extensions of Remarks] [P...\n",
      "\n",
      "Successfully processed 1785 rows.\n",
      "Cleaned CSV saved to: processed_data/immigration_speeches_cleaned.csv\n",
      "Reading CSV file: processed_data/immigration_data_clean.csv\n",
      "Processing 1785 rows...\n",
      "Cleaning full_text column...\n",
      "Saving cleaned data to: processed_data/immigration_data_clean_cleaned.csv\n",
      "\n",
      "Sample of cleaned text:\n",
      "Row 1 (first 100 chars): [Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)] [Daily Digest] [Pages D909...\n",
      "Row 2 (first 100 chars): [Congressional Record Volume 163, Number 141 (Friday, September 1, 2017)] [Extensions of Remarks] [P...\n",
      "\n",
      "Successfully processed 1785 rows.\n",
      "Cleaned CSV saved to: processed_data/immigration_data_clean_cleaned.csv\n",
      "Reading CSV file: processed_data/immigration_speeches_clean.csv\n",
      "Processing 640 rows...\n",
      "Cleaning full_text column...\n",
      "Saving cleaned data to: processed_data/immigration_speeches_clean_cleaned.csv\n",
      "\n",
      "Sample of cleaned text:\n",
      "Row 1 (first 100 chars): [Congressional Record Volume 163, Number 142 (Tuesday, September 5, 2017)] [House] [Pages H6638-H664...\n",
      "Row 2 (first 100 chars): [Congressional Record Volume 163, Number 142 (Tuesday, September 5, 2017)] [House] [Page H6645] From...\n",
      "\n",
      "Successfully processed 640 rows.\n",
      "Cleaned CSV saved to: processed_data/immigration_speeches_clean_cleaned.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'processed_data/immigration_speeches_clean_cleaned.csv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean the full_text column in a CSV file to normalize whitespace\n",
    "def clean_whitespace_in_csv(input_file, output_file=None):\n",
    "\n",
    "    # determine output filename if not provided\n",
    "    if output_file is None:\n",
    "        base, ext = os.path.splitext(input_file)\n",
    "        output_file = f\"{base}_cleaned{ext}\"\n",
    "    \n",
    "    print(f\"Reading CSV file: {input_file}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_file, low_memory=False)\n",
    "        if 'full_text' not in df.columns:\n",
    "            print(\"Warning: 'full_text' column not found in CSV. Available columns:\")\n",
    "            print(\", \".join(df.columns))\n",
    "            return None\n",
    "        \n",
    "        total_rows = len(df)\n",
    "        print(f\"Processing {total_rows} rows...\")\n",
    "        \n",
    "        def clean_text(text):\n",
    "            if pd.isna(text):\n",
    "                return text\n",
    "                \n",
    "            cleaned = re.sub(r'\\s+', ' ', str(text))\n",
    "            cleaned = re.sub(r'\\n\\s*\\n', '\\n', cleaned)\n",
    "            cleaned = cleaned.strip()\n",
    "            return cleaned\n",
    "        \n",
    "        print(\"Cleaning full_text column...\")\n",
    "        df['full_text'] = df['full_text'].apply(clean_text)\n",
    "        \n",
    "        print(f\"Saving cleaned data to: {output_file}\")\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # print sample rows for verification\n",
    "        print(\"\\nSample of cleaned text:\")\n",
    "        for i, row in df.head(2).iterrows():\n",
    "            print(f\"Row {i+1} (first 100 chars): {row['full_text'][:100]}...\")\n",
    "        \n",
    "        print(f\"\\nSuccessfully processed {total_rows} rows.\")\n",
    "        print(f\"Cleaned CSV saved to: {output_file}\")\n",
    "        \n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "# example usage:\n",
    "clean_whitespace_in_csv('processed_data/immigration_speeches.csv')\n",
    "clean_whitespace_in_csv('processed_data/immigration_data_clean.csv')\n",
    "clean_whitespace_in_csv('processed_data/immigration_speeches_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. `immigration_speeches.csv`:\n",
    "   - The raw parsed data from your Congressional Record HTML files\n",
    "   - Contains all the immigration-related speeches and procedural text\n",
    "   - Includes metadata like date, speaker, chamber, etc., along with the full text extracted from HTML files\n",
    "   - This is the initial dataset created by the `process_immigration_files` function\n",
    "\n",
    "2. `immigration_data_clean.csv`:\n",
    "   - Contains all records (both speeches and procedural text) with cleaned and enhanced data\n",
    "   - Includes additional columns like standardized dates, party information, speech categorization\n",
    "   - Adds summary text and metrics like token count and sentence count\n",
    "   - This is the complete dataset after basic preprocessing\n",
    "\n",
    "3. `immigration_speeches_clean.csv`:\n",
    "   - A filtered subset of `immigration_data_clean.csv` containing only actual speeches (no procedural text)\n",
    "   - Uses the `is_speech` flag to filter out non-speech content\n",
    "   - This is the dataset you'd use for analyzing actual Congressional speeches about immigration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
